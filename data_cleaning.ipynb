{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Data Cleaning",
   "id": "3ef6914847727921"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-05T14:13:53.061814Z",
     "start_time": "2025-04-05T14:08:56.594437Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import os\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "# Folder path\n",
    "folder = 'raw_data'\n",
    "\n",
    "# List all files in the folder and filter the .tsv.xz files\n",
    "file = [f for f in os.listdir(folder) if f.endswith('.tsv.xz')]\n",
    "\n",
    "# Assuming there's only one file, get the full file path\n",
    "file_path = os.path.join(folder, file[0])\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv(file_path, sep='\\t', compression='xz')\n",
    "\n",
    "# Clean column names (remove extra spaces)\n",
    "df.columns = df.columns.str.strip()\n",
    "\n",
    "\n",
    "# Function to clean text by removing HTML tags, URLs, and extra spaces\n",
    "def clean_text(text):\n",
    "    if pd.isna(text):  # Return empty string if value is NaN\n",
    "        return ''\n",
    "    text = re.sub(r'<[^>]+>', ' ', text)  # Remove HTML tags\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', ' ', text)  # Remove URLs\n",
    "    text = re.sub(r'&[a-zA-Z0-9#]+;', ' ', text)  # Remove HTML entities\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()  # Reduce multiple spaces\n",
    "    return text\n",
    "\n",
    "\n",
    "def remove_similar_rows(df, threshold=0.995):\n",
    "    # Drop rows where the content_id is the same\n",
    "    df = df.drop_duplicates(subset=['content_id'])\n",
    "\n",
    "\n",
    "\n",
    "    # Clean the content column\n",
    "    df.loc[:, 'content'] = df['content'].apply(clean_text)\n",
    "\n",
    "\n",
    "    # Drop rows where the head is exactly the same\n",
    "    df = df.drop_duplicates(subset=[\"head\"])\n",
    "\n",
    "\n",
    "    # Vectorize the content using TF-IDF\n",
    "    vectorizer = TfidfVectorizer().fit_transform(df['content'])\n",
    "    vectors = vectorizer.toarray()\n",
    "\n",
    "    # Compute cosine similarity matrix\n",
    "    cosine_sim_matrix = cosine_similarity(vectors)\n",
    "\n",
    "    # Identify pairs of articles with similarity above the threshold\n",
    "    similar_pairs = np.where(cosine_sim_matrix > threshold)\n",
    "\n",
    "    # Create a set of indices to drop\n",
    "    indices_to_drop = set()\n",
    "    for i, j in zip(*similar_pairs):\n",
    "        if i != j:\n",
    "            indices_to_drop.add(j)\n",
    "\n",
    "    # Drop the duplicates using .loc to avoid SettingWithCopyWarning\n",
    "    df = df.loc[~df.index.isin(indices_to_drop)]\n",
    "\n",
    "    # Reset index\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "# Drop same or nearly same articles\n",
    "df = remove_similar_rows(df, 0.98)\n",
    "\n",
    "# make the pubtime a df datetime format\n",
    "df['pubtime'] = pd.to_datetime(df['pubtime'])\n",
    "\n",
    "# Create 'cleaned_data' folder if it doesn't exist\n",
    "output_folder = 'cleaned_data'\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# TODO aktuell zu langsam allfällige unnötige Spalten entfernen\n",
    "# Save as Parquet-File in the cleaned_data folder\n",
    "output_file = os.path.join(output_folder, \"cleaned_data.parquet\")\n",
    "df.to_parquet(output_file, engine=\"pyarrow\", index=False)\n"
   ],
   "id": "ec570ddc88f05b59",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/m4/s3tcm9g91yngb_n2nssb3f1w0000gn/T/ipykernel_5424/2633023531.py:77: FutureWarning: In a future version of pandas, parsing datetimes with mixed time zones will raise an error unless `utc=True`. Please specify `utc=True` to opt in to the new behaviour and silence this warning. To create a `Series` with mixed offsets and `object` dtype, please use `apply` and `datetime.datetime.strptime`\n",
      "  df['pubtime'] = pd.to_datetime(df['pubtime'])\n"
     ]
    }
   ],
   "execution_count": 1
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 5,
 "nbformat_minor": 9
}
