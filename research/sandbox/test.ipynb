{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_parquet(\"cleaned_data/cleaned_data.parquet\", engine=\"pyarrow\")"
   ],
   "id": "3f1b66c3f229ec0b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "gefundene_zeilen = df[df[\"head\"].str.contains(\"Statistiken und Vergleich\", case=False, na=False)]\n",
    "\n",
    "# Ausgabe\n",
    "gefundene_zeilen"
   ],
   "id": "c0cb0f2dc5ee2bb5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.manifold import TSNE\n",
    "import plotly.express as px\n",
    "import groq\n",
    "import re\n",
    "import json\n",
    "from collections import Counter\n",
    "import nltk\n",
    "import os\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv(dotenv_path='../.env')\n",
    "\n",
    "# Ensure that you have downloaded the punkt tokenizer for sentence splitting\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Initialize Groq client\n",
    "groq_key = os.getenv(\"GROQ_API_KEY\")\n",
    "client = groq.Groq(api_key=groq_key)\n",
    "\n",
    "def split_text_sentencewise(text, max_length=1000):\n",
    "    \"\"\"Split the text into sentence-wise chunks that do not exceed max_length\"\"\"\n",
    "    if not text or not isinstance(text, str):\n",
    "        return []\n",
    "\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_length = 0\n",
    "\n",
    "    for sentence in sentences:\n",
    "        sentence_length = len(sentence.split())\n",
    "        if current_length + sentence_length > max_length:\n",
    "            if current_chunk:\n",
    "                chunks.append(' '.join(current_chunk))\n",
    "            current_chunk = [sentence]\n",
    "            current_length = sentence_length\n",
    "        else:\n",
    "            current_chunk.append(sentence)\n",
    "            current_length += sentence_length\n",
    "\n",
    "    if current_chunk:\n",
    "        chunks.append(' '.join(current_chunk))\n",
    "\n",
    "    return chunks\n",
    "\n",
    "def analyze_cluster_with_groq(texts, titles, max_chars=2000):\n",
    "    \"\"\"Get a comprehensive analysis of a cluster using Groq\"\"\"\n",
    "    # Create a sample of texts and titles\n",
    "    sample_size = min(5, len(texts))\n",
    "    sample_texts = texts[:sample_size]\n",
    "    sample_titles = titles[:sample_size]\n",
    "\n",
    "    # Create a condensed representation\n",
    "    combined_sample = \"\\n\\n\".join([f\"Titel: {title}\\nText: {text[:300]}...\"\n",
    "                                 for title, text in zip(sample_titles, sample_texts)])\n",
    "\n",
    "    # Trim if too long\n",
    "    if len(combined_sample) > max_chars:\n",
    "        combined_sample = combined_sample[:max_chars] + \"...\"\n",
    "\n",
    "    prompt = f\"\"\"Analysiere diese Gruppe von Nachrichtenartikeln und erstelle eine detaillierte Zusammenfassung:\n",
    "\n",
    "{combined_sample}\n",
    "\n",
    "Liefere folgende Informationen im JSON-Format:\n",
    "1. \"hauptthema\": Ein prägnanter Titel für das Hauptthema (max. 10 Wörter)\n",
    "2. \"zusammenfassung\": Eine klare Zusammenfassung des Themas (50-100 Wörter)\n",
    "3. \"schlüsselwörter\": 5-8 zentrale Begriffe oder Konzepte\n",
    "4. \"entitäten\": Wichtige Personen, Organisationen oder Orte\n",
    "5. \"perspektiven\": Unterschiedliche Blickwinkel oder Meinungen (falls vorhanden)\n",
    "6. \"kontext\": Wichtiger gesellschaftlicher, wirtschaftlicher oder politischer Kontext\n",
    "\n",
    "Antwort nur im JSON-Format.\"\"\"\n",
    "\n",
    "    try:\n",
    "        completion = client.chat.completions.create(\n",
    "            model=\"llama3-70b-8192\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"Du bist ein Experte für Medienanalyse und Themenkategorisierung.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            temperature=0.3,\n",
    "            max_tokens=1000,\n",
    "            response_format={\"type\": \"json_object\"}\n",
    "        )\n",
    "        return json.loads(completion.choices[0].message.content)\n",
    "    except Exception as e:\n",
    "        print(f\"Fehler bei Clusteranalyse: {str(e)}\")\n",
    "        return {\n",
    "            \"hauptthema\": \"Analysefehler\",\n",
    "            \"zusammenfassung\": f\"Fehler bei der Analyse: {str(e)}\",\n",
    "            \"schlüsselwörter\": [],\n",
    "            \"entitäten\": [],\n",
    "            \"perspektiven\": [],\n",
    "            \"kontext\": \"\"\n",
    "        }\n",
    "\n",
    "def optimize_dbscan_params(embeddings, target_clusters=(4, 8)):\n",
    "    \"\"\"Find optimal DBSCAN parameters through grid search\"\"\"\n",
    "    print(\"Suche optimale DBSCAN-Parameter...\")\n",
    "\n",
    "    # Try a range of parameters\n",
    "    eps_values = [0.5, 0.45, 0.4, 0.35, 0.3, 0.25, 0.2, 0.18, 0.16, 0.14, 0.12, 0.1]\n",
    "    min_samples_values = [2, 3, 4, 5, 6]\n",
    "\n",
    "    best_params = None\n",
    "    best_score = float('inf')\n",
    "    best_num_clusters = 0\n",
    "    best_noise_ratio = 1.0\n",
    "\n",
    "    for eps in eps_values:\n",
    "        for min_samples in min_samples_values:\n",
    "            dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "            labels = dbscan.fit_predict(embeddings)\n",
    "\n",
    "            unique_labels = set(labels)\n",
    "            num_clusters = len(unique_labels) - (1 if -1 in unique_labels else 0)\n",
    "            noise_points = list(labels).count(-1)\n",
    "            noise_ratio = noise_points / len(labels)\n",
    "\n",
    "            print(f\"DBSCAN mit eps={eps}, min_samples={min_samples}: {num_clusters} Cluster, {noise_ratio:.1%} Ausreißer\")\n",
    "\n",
    "            # Skip if no clusters or too many noise points\n",
    "            if num_clusters == 0 or num_clusters > target_clusters[1] * 2 or noise_ratio > 0.7:\n",
    "                continue\n",
    "\n",
    "            # Calculate score based on target range and noise\n",
    "            if target_clusters[0] <= num_clusters <= target_clusters[1]:\n",
    "                # Perfect cluster count gets best score\n",
    "                cluster_score = 0\n",
    "            else:\n",
    "                # Penalize being outside target range\n",
    "                cluster_score = min(\n",
    "                    abs(num_clusters - target_clusters[0]),\n",
    "                    abs(num_clusters - target_clusters[1])\n",
    "                )\n",
    "\n",
    "            # Final score is a weighted sum of cluster count penalty and noise ratio\n",
    "            score = cluster_score + (noise_ratio * 2)\n",
    "\n",
    "            if best_params is None or score < best_score:\n",
    "                best_params = (eps, min_samples)\n",
    "                best_score = score\n",
    "                best_num_clusters = num_clusters\n",
    "                best_noise_ratio = noise_ratio\n",
    "                print(f\"  ✓ Neue beste Parameter gefunden (Score: {score:.2f})\")\n",
    "\n",
    "    if best_params:\n",
    "        print(f\"Optimale Parameter: eps={best_params[0]}, min_samples={best_params[1]}\")\n",
    "        print(f\"  → {best_num_clusters} Cluster mit {best_noise_ratio:.1%} Ausreißern\")\n",
    "        return best_params\n",
    "    else:\n",
    "        print(\"Keine optimalen Parameter gefunden. Verwende Standardwerte.\")\n",
    "        return (0.3, 3)  # Conservative default\n",
    "\n",
    "def df_plot_dbscan(df, target_clusters=(4, 8)):\n",
    "    \"\"\"Improved DBSCAN clustering with better visualization and analysis\"\"\"\n",
    "    print(f\"Clustering {len(df)} Artikel...\")\n",
    "\n",
    "    # Weight headlines more in the combined text (3x)\n",
    "    df.loc[:, \"combined_text\"] = df[\"head\"].str.repeat(3) + \" \" + df[\"content\"]\n",
    "\n",
    "    # Use a better model for German text\n",
    "    print(\"Erstelle Embeddings...\")\n",
    "    model = SentenceTransformer('paraphrase-multilingual-mpnet-base-v2')\n",
    "    embeddings = model.encode(df['combined_text'].tolist(), show_progress_bar=True)\n",
    "\n",
    "    # Find optimal parameters\n",
    "    eps, min_samples = optimize_dbscan_params(embeddings, target_clusters)\n",
    "\n",
    "    # Apply DBSCAN with best parameters\n",
    "    dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "    df.loc[:, 'cluster'] = dbscan.fit_predict(embeddings)\n",
    "\n",
    "    # Get cluster statistics\n",
    "    unique_labels = set(df['cluster'])\n",
    "    num_clusters = len(unique_labels) - (1 if -1 in unique_labels else 0)\n",
    "    noise_points = list(df['cluster']).count(-1)\n",
    "    print(f\"Clustering ergab {num_clusters} Cluster und {noise_points} Ausreißer ({noise_points/len(df):.1%})\")\n",
    "\n",
    "    # Create t-SNE visualization\n",
    "    print(\"Erstelle t-SNE Visualisierung...\")\n",
    "    perplexity = min(30, max(5, len(df) // 15))\n",
    "    tsne = TSNE(n_components=2, random_state=42, perplexity=perplexity)\n",
    "    tsne_results = tsne.fit_transform(embeddings)\n",
    "    df.loc[:, 'tsne_x'] = tsne_results[:, 0]\n",
    "    df.loc[:, 'tsne_y'] = tsne_results[:, 1]\n",
    "\n",
    "    # Filter out noise points\n",
    "    filtered_df = df[df['cluster'] >= 0]\n",
    "\n",
    "    # Create plot\n",
    "    fig = px.scatter(\n",
    "        filtered_df,\n",
    "        x='tsne_x',\n",
    "        y='tsne_y',\n",
    "        color='cluster',\n",
    "        hover_data=['head'],\n",
    "        title=f\"Themencluster ({num_clusters} Cluster)\",\n",
    "        color_discrete_sequence=px.colors.qualitative.Bold,\n",
    "        opacity=0.7\n",
    "    )\n",
    "\n",
    "    fig.update_layout(\n",
    "        legend_title_text='Thema',\n",
    "        xaxis=dict(showgrid=False, zeroline=False, showticklabels=False, title=\"\"),\n",
    "        yaxis=dict(showgrid=False, zeroline=False, showticklabels=False, title=\"\")\n",
    "    )\n",
    "\n",
    "    # Add cluster annotations\n",
    "    for cluster_id in filtered_df['cluster'].unique():\n",
    "        cluster_df = filtered_df[filtered_df['cluster'] == cluster_id]\n",
    "        center_x = cluster_df['tsne_x'].mean()\n",
    "        center_y = cluster_df['tsne_y'].mean()\n",
    "\n",
    "        fig.add_annotation(\n",
    "            x=center_x,\n",
    "            y=center_y,\n",
    "            text=f\"Cluster {cluster_id}\",\n",
    "            showarrow=False,\n",
    "            font=dict(size=14, color=\"black\"),\n",
    "            bgcolor=\"rgba(255, 255, 255, 0.7)\",\n",
    "            bordercolor=\"black\",\n",
    "            borderwidth=1,\n",
    "            borderpad=4\n",
    "        )\n",
    "\n",
    "    fig.show()\n",
    "\n",
    "    # Analyze clusters by size\n",
    "    clusters_by_size = filtered_df['cluster'].value_counts().sort_values(ascending=False).index\n",
    "\n",
    "    for cluster_id in clusters_by_size:\n",
    "        cluster_data = filtered_df[filtered_df['cluster'] == cluster_id]\n",
    "        cluster_size = len(cluster_data)\n",
    "\n",
    "        # Skip tiny clusters\n",
    "        if cluster_size < 2:\n",
    "            continue\n",
    "\n",
    "        print(f\"\\n\\n=== Cluster {cluster_id} (Größe: {cluster_size}) ===\")\n",
    "\n",
    "        # Analyze cluster with Groq\n",
    "        print(\"Analysiere Cluster...\")\n",
    "        analysis = analyze_cluster_with_groq(\n",
    "            cluster_data[\"combined_text\"].tolist(),\n",
    "            cluster_data[\"head\"].tolist()\n",
    "        )\n",
    "\n",
    "        # Print analysis results\n",
    "        print(f\"Hauptthema: {analysis.get('hauptthema', 'Nicht verfügbar')}\")\n",
    "        print(f\"Zusammenfassung: {analysis.get('zusammenfassung', 'Nicht verfügbar')}\")\n",
    "        print(f\"Schlüsselwörter: {', '.join(analysis.get('schlüsselwörter', []))}\")\n",
    "        print(f\"Entitäten: {', '.join(analysis.get('entitäten', []))}\")\n",
    "\n",
    "        # Print additional insights if available\n",
    "        if analysis.get('perspektiven'):\n",
    "            print(f\"Perspektiven: {analysis.get('perspektiven')}\")\n",
    "        if analysis.get('kontext'):\n",
    "            print(f\"Kontext: {analysis.get('kontext')}\")\n",
    "\n",
    "        # Show sample headlines\n",
    "        print(\"\\nSchlagzeilen:\")\n",
    "        for title in cluster_data['head'].tolist()[:5]:\n",
    "            print(f\"• {title}\")\n",
    "\n",
    "    return filtered_df\n",
    "\n",
    "def df_cluster_news(df, target_clusters=(4, 8)):\n",
    "    \"\"\"Wrapper for clustering functionality\"\"\"\n",
    "    return df_plot_dbscan(df, target_clusters)"
   ],
   "id": "b568b2f4cb10b669"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def process_by_date(df, target_clusters=(4, 8)):\n",
    "    \"\"\"Process the dataframe either as a whole or by date if pubtime column is available\"\"\"\n",
    "    if 'pubtime' in df.columns:\n",
    "        # Convert to datetime if it isn't already\n",
    "        if not pd.api.types.is_datetime64_any_dtype(df['pubtime']):\n",
    "            df['pubtime'] = pd.to_datetime(df['pubtime'])\n",
    "\n",
    "        # Find all dates in the dataframe\n",
    "        all_dates = sorted(df['pubtime'].dt.date.unique())\n",
    "\n",
    "        # Process each date separately\n",
    "        for date in all_dates:\n",
    "            date_df = df[df['pubtime'].dt.date == date]\n",
    "            if len(date_df) > 0:\n",
    "                print(f\"\\n\\n{'='*50}\")\n",
    "                print(f\"=== Themen für {date} ({len(date_df)} Artikel) ===\")\n",
    "                print(f\"{'='*50}\")\n",
    "                df_cluster_news(date_df, target_clusters)\n",
    "            else:\n",
    "                print(f\"Keine Artikel für {date} verfügbar.\")\n",
    "    else:\n",
    "        print(f\"\\n\\n{'='*50}\")\n",
    "        print(f\"=== Analyse des gesamten Datensatzes ({len(df)} Artikel) ===\")\n",
    "        print(f\"{'='*50}\")\n",
    "        df_cluster_news(df, target_clusters)\n",
    "\n",
    "# Example usage with the dataframe\n",
    "# process_by_date(df, target_clusters=(3, 7))"
   ],
   "id": "551e664c19d2862c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# For all articles\n",
    "process_by_date(df)\n",
    "\n",
    "# Or with custom cluster targets\n",
    "# process_by_date(df, target_clusters=(3, 6))"
   ],
   "id": "a6eeb95747793dc0"
  }
 ],
 "metadata": {},
 "nbformat": 5,
 "nbformat_minor": 9
}
