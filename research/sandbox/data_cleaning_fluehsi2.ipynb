{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "### Data Cleaning",
   "id": "c86a6316730807"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "# Load the file\n",
    "file_path = '58ecabf6-9618-407a-86a5-0d162c326ed8__2025_03_28T07_03_57.tsv.xz'\n",
    "df = pd.read_csv(file_path, sep='\\t', compression='xz')\n",
    "\n",
    "# Clean column names (remove extra spaces)\n",
    "df.columns = df.columns.str.strip()\n",
    "\n",
    "\n",
    "# Function to clean text by removing HTML tags, URLs, and extra spaces\n",
    "def clean_text(text):\n",
    "    if pd.isna(text):  # Return empty string if value is NaN\n",
    "        return ''\n",
    "    text = re.sub(r'<[^>]+>', ' ', text)  # Remove HTML tags\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', ' ', text)  # Remove URLs\n",
    "    text = re.sub(r'&[a-zA-Z0-9#]+;', ' ', text)  # Remove HTML entities\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()  # Reduce multiple spaces\n",
    "    return text\n",
    "\n",
    "\n",
    "def remove_similar_rows(df, threshold=0.995):\n",
    "    # Drop rows where the content_id is the same\n",
    "    df = df.drop_duplicates(subset=['content_id'])\n",
    "\n",
    "\n",
    "\n",
    "    # Clean the content column\n",
    "    df.loc[:, 'content'] = df['content'].apply(clean_text)\n",
    "\n",
    "\n",
    "    # Drop rows where the head is exactly the same\n",
    "    df = df.drop_duplicates(subset=[\"head\"])\n",
    "\n",
    "\n",
    "    # Vectorize the content using TF-IDF\n",
    "    vectorizer = TfidfVectorizer().fit_transform(df['content'])\n",
    "    vectors = vectorizer.toarray()\n",
    "\n",
    "    # Compute cosine similarity matrix\n",
    "    cosine_sim_matrix = cosine_similarity(vectors)\n",
    "\n",
    "    # Identify pairs of articles with similarity above the threshold\n",
    "    similar_pairs = np.where(cosine_sim_matrix > threshold)\n",
    "\n",
    "    # Create a set of indices to drop\n",
    "    indices_to_drop = set()\n",
    "    for i, j in zip(*similar_pairs):\n",
    "        if i != j:\n",
    "            indices_to_drop.add(j)\n",
    "\n",
    "    # Drop the duplicates using .loc to avoid SettingWithCopyWarning\n",
    "    df = df.loc[~df.index.isin(indices_to_drop)]\n",
    "\n",
    "    # Reset index\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "# Drop same or nearly same articles\n",
    "df = remove_similar_rows(df, 0.65)\n",
    "\n",
    "# make the pubtime a df datetime format\n",
    "df['pubtime'] = pd.to_datetime(df['pubtime'])\n",
    "\n",
    "df.shape\n",
    "\n",
    "# save as Parquet-File\n",
    "df.to_parquet(\"bereinigte_daten.parquet\", engine=\"pyarrow\", index=False)\n",
    "\n",
    "\n"
   ],
   "id": "fa5e39e9f3171544"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.manifold import TSNE\n",
    "import plotly.express as px\n",
    "import torch\n",
    "\n",
    "def df_plot_dbscan(df, target_clusters=(4, 6)):\n",
    "    # Überprüfe die Größe des DataFrames und setze die DBSCAN-Parameter entsprechend\n",
    "    if len(df) <= 400:\n",
    "        eps = 0.88\n",
    "        min_samples = 4\n",
    "    else:\n",
    "        eps = 0.85\n",
    "        min_samples = 6\n",
    "\n",
    "    # Kombiniere den Header und den bereinigten Inhalt\n",
    "    df['combined_text'] = df['head'] + ' ' + df['content']\n",
    "\n",
    "    # Lade das vortrainierte LeoLM Modell und den Tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained('LeoLM/leo-hessianai-13b')\n",
    "    model = AutoModel.from_pretrained('LeoLM/leo-hessianai-13b')\n",
    "\n",
    "    # Wandle die Texte in Embeddings um\n",
    "    embeddings = []\n",
    "    for text in df['combined_text'].tolist():\n",
    "        inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True, max_length=8000)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        embeddings.append(outputs.last_hidden_state.mean(dim=1).squeeze().numpy())\n",
    "\n",
    "    # DBSCAN Clustering mit den aktuellen Hyperparametern\n",
    "    dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "    df['dbscan_cluster'] = dbscan.fit_predict(embeddings)\n",
    "\n",
    "    # Überprüfe, wie viele Cluster es gibt\n",
    "    num_clusters = len(df['dbscan_cluster'].unique()) - (1 if -1 in df['dbscan_cluster'].unique() else 0)\n",
    "\n",
    "    # Falls weniger als 4 Cluster, erhöhen wir eps, falls mehr als 6 Cluster, verringern wir eps\n",
    "    while num_clusters < target_clusters[0] or num_clusters > target_clusters[1]:\n",
    "        if num_clusters < target_clusters[0]:\n",
    "            eps += 0.025  # Erhöhe eps\n",
    "        elif num_clusters > target_clusters[1]:\n",
    "            eps -= 0.025  # Verringere eps\n",
    "\n",
    "        # Führe DBSCAN erneut aus\n",
    "        dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "        df['dbscan_cluster'] = dbscan.fit_predict(embeddings)\n",
    "\n",
    "        # Überprüfe die Anzahl der Cluster erneut\n",
    "        num_clusters = len(df['dbscan_cluster'].unique()) - (1 if -1 in df['dbscan_cluster'].unique() else 0)\n",
    "\n",
    "    # Anwendung von t-SNE zur Reduktion auf 2 Dimensionen\n",
    "    tsne = TSNE(n_components=2, random_state=42)\n",
    "    tsne_results = tsne.fit_transform(embeddings)\n",
    "\n",
    "    # Füge die t-SNE-Ergebnisse und die Cluster zum DataFrame hinzu\n",
    "    df['tsne_x'] = tsne_results[:, 0]\n",
    "    df['tsne_y'] = tsne_results[:, 1]\n",
    "\n",
    "    # Erstelle eine interaktive Plotly-Visualisierung für DBSCAN\n",
    "    fig_dbscan = px.scatter(df[df['dbscan_cluster'] >= 0], x='tsne_x', y='tsne_y', color='dbscan_cluster', hover_data=['head'])\n",
    "\n",
    "    # Zeige die Cluster-Titel an\n",
    "    for cluster_id in range(len(df['dbscan_cluster'].unique()) - 1):\n",
    "        cluster_data = df[df['dbscan_cluster'] == cluster_id]\n",
    "        print(f\"Cluster {cluster_id} heads:\")\n",
    "        for title in cluster_data['head'].tolist():\n",
    "            print(\"   \" + title)\n",
    "\n",
    "    # Zeige die DBSCAN-Grafik an\n",
    "    fig_dbscan.show()\n",
    "\n",
    "# Beispielhafte Anwendung\n",
    "# Angenommen, df ist Ihr DataFrame mit den Spalten 'head', 'content' und 'pubtime'\n",
    "# unique_dates = df['pubtime'].dt.date.unique()\n",
    "# for date in unique_dates:\n",
    "#     subset_df = df[df['pubtime'].dt.date == date]\n",
    "#     print(f\"Topics of {date}:\")\n",
    "#     df_plot_dbscan(subset_df, target_clusters=(3, 6))\n"
   ],
   "id": "4c9361aadd5b986a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "df_plot_dbscan(df)",
   "id": "bb175abcf09f2154"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Analyse von Dokumenten an Tagen mit geringerer Nachrichtenaktivität: bsp Sonntagen",
   "id": "1830752ef31f8d9a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 16.03 nur 179 ansonsten um die 600\n",
    "subset_df = df[df['pubtime'].dt.date == pd.to_datetime('2025-03-12').date()]\n",
    "subset_df.shape"
   ],
   "id": "ee6fee7dd832fbc1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "from transformers import LongformerModel, LongformerTokenizer\n",
    "import torch\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.manifold import TSNE\n",
    "import plotly.express as px\n",
    "\n",
    "def df_plot_dbscan(df, eps=0.85, min_samples=6):\n",
    "    # Kombiniere den Header und den bereinigten Inhalt\n",
    "    df.loc[:, \"combined_text\"] = df[\"head\"] + \" \" + df[\"content\"]\n",
    "\n",
    "    # Lade das Longformer-Modell und den Tokenizer\n",
    "    model_name = 'allenai/longformer-base-4096'\n",
    "    tokenizer = LongformerTokenizer.from_pretrained(model_name)\n",
    "    model = LongformerModel.from_pretrained(model_name)\n",
    "\n",
    "    # Funktion zur Erstellung von Embeddings mit Longformer\n",
    "    def get_longformer_embedding(text):\n",
    "        inputs = tokenizer(text, return_tensors='pt', truncation=True, max_length=4096, padding=True)\n",
    "        # Verwende CPU/GPU für die Berechnung\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        inputs = {key: value.to(device) for key, value in inputs.items()}\n",
    "        model.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "\n",
    "        # Berechne den Durchschnitt der Hidden States als Embedding\n",
    "        return outputs.last_hidden_state.mean(dim=1).squeeze().cpu().numpy()\n",
    "\n",
    "    # Wandle die Texte in Embeddings um\n",
    "    df.loc[:, \"embeddings\"] = df[\"combined_text\"].apply(get_longformer_embedding)\n",
    "    embeddings = list(df[\"embeddings\"])\n",
    "\n",
    "    # DBSCAN Clustering mit den angegebenen Parametern\n",
    "    dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "    df.loc[:, 'dbscan_cluster'] = dbscan.fit_predict(embeddings)\n",
    "\n",
    "    # Anwendung von t-SNE zur Reduktion auf 2 Dimensionen\n",
    "    tsne = TSNE(n_components=2, random_state=42)\n",
    "    tsne_results = tsne.fit_transform(embeddings)\n",
    "\n",
    "    # Füge die t-SNE-Ergebnisse und die Cluster zu den DataFrame hinzu\n",
    "    df.loc[:, 'tsne_x'] = tsne_results[:, 0]\n",
    "    df.loc[:, 'tsne_y'] = tsne_results[:, 1]\n",
    "\n",
    "    # Erstelle eine interaktive Plotly-Visualisierung für DBSCAN\n",
    "    fig_dbscan = px.scatter(df[df[\"dbscan_cluster\"] >= 0], x='tsne_x', y='tsne_y', color='dbscan_cluster', hover_data=['head'])\n",
    "\n",
    "    # Zeige die Cluster-Titel an\n",
    "    for cluster_id in sorted(df['dbscan_cluster'].unique()):\n",
    "        if cluster_id == -1:\n",
    "            continue  # Überspringe den Noise-Cluster\n",
    "        cluster_data = df[df['dbscan_cluster'] == cluster_id]\n",
    "        print(f\"Cluster {cluster_id} heads:\")\n",
    "        for title in cluster_data['head'].tolist():\n",
    "            print(\"   \" + title)\n",
    "\n",
    "    # Zeige die DBSCAN Grafik an\n",
    "    fig_dbscan.show()\n"
   ],
   "id": "2719845b1e021d2f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "df_plot_dbscan(subset_df)",
   "id": "191019168a8c41da"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Algorithmus Tägliche Themenanalyse basierend auf Nachrichtenaktivität: DBSCAN-Cluster für den {date}",
   "id": "989c296306edd2c0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.manifold import TSNE\n",
    "import plotly.express as px\n",
    "\n",
    "import re\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "def split_into_sentence_windows(text, window_size=3, overlap=1):\n",
    "    \"\"\" Zerlegt den Text in vollständige Sätze und erstellt überlappende Fenster. \"\"\"\n",
    "    sentences = re.split(r'(?<=[.!?])\\s+', text)  # Split nach Satzzeichen\n",
    "    return [\n",
    "        \" \".join(sentences[i:i + window_size])\n",
    "        for i in range(0, len(sentences) - window_size + 1, window_size - overlap)\n",
    "    ] if len(sentences) >= window_size else [\" \".join(sentences)]\n",
    "\n",
    "def is_outlier(row, means, stds, max_distance_factor):\n",
    "    \"\"\" Prüft, ob ein Punkt als Ausreißer gilt. \"\"\"\n",
    "    cluster_id = row['dbscan_cluster']\n",
    "    if cluster_id not in means.index:\n",
    "        return False\n",
    "    dist_x = abs(row['tsne_x'] - means.loc[cluster_id, 'tsne_x'])\n",
    "    dist_y = abs(row['tsne_y'] - means.loc[cluster_id, 'tsne_y'])\n",
    "    return dist_x > max_distance_factor * stds.loc[cluster_id, 'tsne_x'] or \\\n",
    "           dist_y > max_distance_factor * stds.loc[cluster_id, 'tsne_y']\n",
    "\n",
    "def df_plot_dbscan(df, target_clusters=(4, 6), max_distance_factor=1.5):\n",
    "    if len(df) <= 400:\n",
    "        eps = 0.05\n",
    "        min_samples = 4\n",
    "    else:\n",
    "        eps = 0.04\n",
    "        min_samples = 6\n",
    "\n",
    "    df.loc[:, \"combined_text\"] = df[\"head\"] + \" \" + df[\"content\"]\n",
    "\n",
    "    # Erzeuge Sätze mit Überlappung für jeden Eintrag\n",
    "    df[\"sentence_windows\"] = df[\"combined_text\"].apply(lambda text: split_into_sentence_windows(text))\n",
    "\n",
    "    # Flach die Liste ab und erzeuge ein DataFrame mit den Fenstern\n",
    "    expanded_df = df.explode(\"sentence_windows\").reset_index(drop=True)\n",
    "\n",
    "    # Berechne Embeddings für die Satzfenster\n",
    "    model = SentenceTransformer('all-MiniLM-L12-v2')\n",
    "    embeddings = model.encode(expanded_df['sentence_windows'].tolist())\n",
    "\n",
    "    # Führe DBSCAN-Clustering durch\n",
    "    dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "    expanded_df.loc[:, 'dbscan_cluster'] = dbscan.fit_predict(embeddings)\n",
    "\n",
    "    # Aggregiere das Cluster-Ergebnis zurück auf das Original-DataFrame (Mehrheitsentscheidung)\n",
    "    expanded_df = expanded_df.reset_index()  # Index in eine Spalte umwandeln\n",
    "    df_clusters = expanded_df.groupby(\"index\")[\"dbscan_cluster\"].agg(lambda x: x.value_counts().idxmax())\n",
    "\n",
    "    df[\"dbscan_cluster\"] = df_clusters\n",
    "\n",
    "    num_clusters = len(df['dbscan_cluster'].unique()) - (1 if -1 in df['dbscan_cluster'].unique() else 0)\n",
    "\n",
    "    while num_clusters < target_clusters[0] or num_clusters > target_clusters[1]:\n",
    "        if num_clusters < target_clusters[0]:\n",
    "            eps += 0.025\n",
    "        elif num_clusters > target_clusters[1]:\n",
    "            eps -= 0.025\n",
    "\n",
    "        dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "        df[\"dbscan_cluster\"] = dbscan.fit_predict(embeddings)\n",
    "        num_clusters = len(df['dbscan_cluster'].unique()) - (1 if -1 in df['dbscan_cluster'].unique() else 0)\n",
    "\n",
    "    # TSNE für die Visualisierung\n",
    "    tsne = TSNE(n_components=2, random_state=42)\n",
    "    tsne_results = tsne.fit_transform(embeddings)\n",
    "    df.loc[:, 'tsne_x'] = tsne_results[:, 0]\n",
    "    df.loc[:, 'tsne_y'] = tsne_results[:, 1]\n",
    "\n",
    "    # Entferne Ausreißer\n",
    "    filtered_df = df[df[\"dbscan_cluster\"] >= 0]\n",
    "    cluster_means = filtered_df.groupby(\"dbscan_cluster\")[[\"tsne_x\", \"tsne_y\"]].mean()\n",
    "    cluster_stds = filtered_df.groupby(\"dbscan_cluster\")[[\"tsne_x\", \"tsne_y\"]].std()\n",
    "\n",
    "    filtered_df = filtered_df[~filtered_df.apply(lambda row: is_outlier(row, cluster_means, cluster_stds, max_distance_factor), axis=1)]\n",
    "\n",
    "    # Erstelle Scatter-Plot\n",
    "    fig_dbscan = px.scatter(filtered_df, x='tsne_x', y='tsne_y', color='dbscan_cluster', hover_data=['head'])\n",
    "\n",
    "    for cluster_id in sorted(filtered_df[\"dbscan_cluster\"].unique()):\n",
    "        cluster_data = filtered_df[filtered_df['dbscan_cluster'] == cluster_id]\n",
    "        print(f\"Cluster {cluster_id} heads:\")\n",
    "        for title in cluster_data['head'].tolist():\n",
    "            print(\"   \" + title)\n",
    "\n",
    "    fig_dbscan.show()\n",
    "\n",
    "unique_dates = df['pubtime'].dt.date.unique()\n",
    "\n",
    "for date in unique_dates:\n",
    "    subset_df = df[df['pubtime'].dt.date == date]\n",
    "    print(f\"Topics of {date}:\")\n",
    "    df_plot_dbscan(subset_df, target_clusters=(3, 6))"
   ],
   "id": "da3abd1137ee9ed0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "1832ec9ea558c499"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.manifold import TSNE\n",
    "import plotly.express as px\n",
    "import groq\n",
    "import re\n",
    "import json\n",
    "from collections import Counter\n",
    "import nltk\n",
    "\n",
    "# Ensure that you have downloaded the punkt tokenizer for sentence splitting\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Initialize Groq client\n",
    "client = groq.Groq(api_key=\"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\")\n",
    "\n",
    "def split_text_sentencewise(text, max_length=1000):\n",
    "    \"\"\"Split the text into sentence-wise chunks that do not exceed max_length\"\"\"\n",
    "    sentences = nltk.sent_tokenize(text)  # Tokenize into sentences\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_length = 0\n",
    "\n",
    "    for sentence in sentences:\n",
    "        sentence_length = len(sentence.split())  # Count words (approximate tokens)\n",
    "\n",
    "        # If adding this sentence exceeds the max length, start a new chunk\n",
    "        if current_length + sentence_length > max_length:\n",
    "            if current_chunk:\n",
    "                chunks.append(' '.join(current_chunk))\n",
    "            current_chunk = [sentence]\n",
    "            current_length = sentence_length\n",
    "        else:\n",
    "            current_chunk.append(sentence)\n",
    "            current_length += sentence_length\n",
    "\n",
    "    # Add the last chunk if any\n",
    "    if current_chunk:\n",
    "        chunks.append(' '.join(current_chunk))\n",
    "\n",
    "    return chunks\n",
    "\n",
    "def parse_json_response(response):\n",
    "    \"\"\"Extract JSON from Groq response\"\"\"\n",
    "    try:\n",
    "        json_str = re.search(r'\\{.*\\}', response, re.DOTALL)\n",
    "        if json_str:\n",
    "            return json.loads(json_str.group())\n",
    "        return {}\n",
    "    except json.JSONDecodeError:\n",
    "        return {}\n",
    "\n",
    "def get_wikipedia_suggestions(text):\n",
    "    \"\"\"Get Wikipedia suggestions from Groq for a single text, ensuring the titles exist on Wikipedia\"\"\"\n",
    "\n",
    "    # Split the text into chunks sentence-wise to avoid exceeding LLM input size limit\n",
    "    chunks = split_text_sentencewise(text)\n",
    "\n",
    "    all_titles = []\n",
    "\n",
    "    for chunk in chunks:\n",
    "        prompt = (\n",
    "            \"Gib 3-5 relevante Wikipedia-Artikel-Titel für diesen Text als JSON. \"\n",
    "            \"Die Titel müssen echte Wikipedia-Artikel sein, d.h. sie müssen genau übereinstimmen. \"\n",
    "            \"Format: {'titles': ['Artikel1', 'Artikel2']}. Nur exakte Artikelnamen, keine Vermutungen oder Platzhalter:\\n\\n\"\n",
    "            f\"{chunk}\"  # Each chunk of the text\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            completion = client.chat.completions.create(\n",
    "                model=\"llama3-70b-8192\",\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"Du bist ein hilfreicher Assistent für Wikipedia-Recherche.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ],\n",
    "                temperature=0.5,\n",
    "                max_tokens=200,\n",
    "                response_format={\"type\": \"json_object\"}\n",
    "            )\n",
    "            response = completion.choices[0].message.content\n",
    "            titles = parse_json_response(response).get('titles', [])\n",
    "            all_titles.extend(titles)\n",
    "        except Exception as e:\n",
    "            print(f\"API Fehler bei Chunk: {str(e)}\")\n",
    "\n",
    "    return all_titles\n",
    "\n",
    "def get_common_suggestions(cluster_texts):\n",
    "    \"\"\"Analyze multiple texts to find common Wikipedia articles\"\"\"\n",
    "    all_articles = []\n",
    "\n",
    "    for text in cluster_texts:\n",
    "        articles = get_wikipedia_suggestions(text)\n",
    "        if articles:\n",
    "            all_articles.extend(articles)\n",
    "\n",
    "    if not all_articles:\n",
    "        return \"Keine relevanten Artikel gefunden\"\n",
    "\n",
    "    # Count and filter articles\n",
    "    counter = Counter(all_articles)\n",
    "    min_occurrences = max(2, len(cluster_texts) // 3)\n",
    "    common_articles = [\n",
    "        f\"{art} ({cnt}x)\"\n",
    "        for art, cnt in counter.most_common()\n",
    "        if cnt >= min_occurrences\n",
    "    ]\n",
    "\n",
    "    return \"\\n\".join(common_articles[:5]) if common_articles else \"Keine konsistenten Artikel\"\n",
    "\n",
    "import re\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "def split_into_sentence_windows(text, window_size=3, overlap=1):\n",
    "    \"\"\" Zerlegt den Text in vollständige Sätze und erstellt überlappende Fenster. \"\"\"\n",
    "    sentences = re.split(r'(?<=[.!?])\\s+', text)  # Split nach Satzzeichen\n",
    "    return [\n",
    "        \" \".join(sentences[i:i + window_size])\n",
    "        for i in range(0, len(sentences) - window_size + 1, window_size - overlap)\n",
    "    ] if len(sentences) >= window_size else [\" \".join(sentences)]\n",
    "\n",
    "def is_outlier(row, means, stds, max_distance_factor):\n",
    "    \"\"\" Prüft, ob ein Punkt als Ausreißer gilt. \"\"\"\n",
    "    cluster_id = row['dbscan_cluster']\n",
    "    if cluster_id not in means.index:\n",
    "        return False\n",
    "    dist_x = abs(row['tsne_x'] - means.loc[cluster_id, 'tsne_x'])\n",
    "    dist_y = abs(row['tsne_y'] - means.loc[cluster_id, 'tsne_y'])\n",
    "    return dist_x > max_distance_factor * stds.loc[cluster_id, 'tsne_x'] or \\\n",
    "           dist_y > max_distance_factor * stds.loc[cluster_id, 'tsne_y']\n",
    "\n",
    "def df_plot_dbscan(df, target_clusters=(4, 6), max_distance_factor=1.5):\n",
    "    if len(df) <= 400:\n",
    "        eps = 0.05\n",
    "        min_samples = 4\n",
    "    else:\n",
    "        eps = 0.04\n",
    "        min_samples = 6\n",
    "\n",
    "    # Create combined text column\n",
    "    df = df.copy()  # Avoid SettingWithCopyWarning\n",
    "    df.loc[:, \"combined_text\"] = df[\"head\"] + \" \" + df[\"content\"]\n",
    "\n",
    "    # Create sentence windows for each entry\n",
    "    df.loc[:, \"sentence_windows\"] = df[\"combined_text\"].apply(lambda text: split_into_sentence_windows(text))\n",
    "\n",
    "    # Expand the DataFrame with sentence windows\n",
    "    expanded_df = df.explode(\"sentence_windows\").reset_index()\n",
    "\n",
    "    # Calculate embeddings for the sentence windows\n",
    "    model = SentenceTransformer('all-MiniLM-L12-v2')\n",
    "    embeddings = model.encode(expanded_df['sentence_windows'].tolist())\n",
    "\n",
    "    # Perform DBSCAN clustering\n",
    "    dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "    expanded_df.loc[:, 'dbscan_cluster'] = dbscan.fit_predict(embeddings)\n",
    "\n",
    "    # Aggregate cluster results back to original DataFrame (majority voting)\n",
    "    df_clusters = expanded_df.groupby(\"index\")[\"dbscan_cluster\"].agg(lambda x: x.value_counts().idxmax())\n",
    "    df.loc[:, \"dbscan_cluster\"] = df_clusters\n",
    "\n",
    "    # Adjust eps to reach target cluster range\n",
    "    num_clusters = len(df['dbscan_cluster'].unique()) - (1 if -1 in df['dbscan_cluster'].unique() else 0)\n",
    "\n",
    "    while num_clusters < target_clusters[0] or num_clusters > target_clusters[1]:\n",
    "        if num_clusters < target_clusters[0]:\n",
    "            eps += 0.025\n",
    "        elif num_clusters > target_clusters[1]:\n",
    "            eps -= 0.025\n",
    "\n",
    "        dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "        expanded_df.loc[:, 'dbscan_cluster'] = dbscan.fit_predict(embeddings)\n",
    "        df_clusters = expanded_df.groupby(\"index\")[\"dbscan_cluster\"].agg(lambda x: x.value_counts().idxmax())\n",
    "        df.loc[:, \"dbscan_cluster\"] = df_clusters\n",
    "        num_clusters = len(df['dbscan_cluster'].unique()) - (1 if -1 in df['dbscan_cluster'].unique() else 0)\n",
    "\n",
    "    # TSNE for visualization\n",
    "    tsne = TSNE(n_components=2, random_state=42)\n",
    "    tsne_results = tsne.fit_transform(embeddings)\n",
    "    expanded_df.loc[:, 'tsne_x'] = tsne_results[:, 0]\n",
    "    expanded_df.loc[:, 'tsne_y'] = tsne_results[:, 1]\n",
    "\n",
    "    # Filter out noise points (cluster = -1)\n",
    "    filtered_df = expanded_df[expanded_df[\"dbscan_cluster\"] >= 0]\n",
    "\n",
    "    # Calculate cluster statistics\n",
    "    if not filtered_df.empty:\n",
    "        cluster_means = filtered_df.groupby(\"dbscan_cluster\")[[\"tsne_x\", \"tsne_y\"]].mean()\n",
    "        cluster_stds = filtered_df.groupby(\"dbscan_cluster\")[[\"tsne_x\", \"tsne_y\"]].std()\n",
    "\n",
    "        # Remove outliers\n",
    "        filtered_df = filtered_df[\n",
    "            ~filtered_df.apply(\n",
    "                lambda row: is_outlier(row, cluster_means, cluster_stds, max_distance_factor),\n",
    "                axis=1\n",
    "            )\n",
    "        ]\n",
    "\n",
    "        # Create scatter plot\n",
    "        fig_dbscan = px.scatter(\n",
    "            filtered_df,\n",
    "            x='tsne_x',\n",
    "            y='tsne_y',\n",
    "            color='dbscan_cluster',\n",
    "            hover_data=['sentence_windows']\n",
    "        )\n",
    "        fig_dbscan.show()\n",
    "\n",
    "        # Print cluster information\n",
    "        for cluster_id in sorted(filtered_df[\"dbscan_cluster\"].unique()):\n",
    "            cluster_data = filtered_df[filtered_df['dbscan_cluster'] == cluster_id]\n",
    "            print(f\"\\nCluster {cluster_id} (Size: {len(cluster_data)})\")\n",
    "\n",
    "            # Get common Wikipedia articles\n",
    "            suggestions = get_common_suggestions(\n",
    "                cluster_data[\"sentence_windows\"].tolist()\n",
    "            )\n",
    "            print(\"\\nHäufigste Wikipedia-Artikel:\")\n",
    "            print(suggestions)\n",
    "# Process by date\n",
    "if 'pubtime' in df.columns:\n",
    "    for date in sorted(df['pubtime'].dt.date.unique()):\n",
    "        date_df = df[df['pubtime'].dt.date == date]\n",
    "        print(f\"\\n\\n=== Themen für {date} ===\")\n",
    "        df_plot_dbscan(date_df)\n",
    "\n"
   ],
   "id": "24f5ccb5e0ac0da5"
  }
 ],
 "metadata": {},
 "nbformat": 5,
 "nbformat_minor": 9
}
