{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_parquet(\"cleaned_data/cleaned_data.parquet\", engine=\"pyarrow\")"
   ],
   "id": "5c38df505e483dfd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "gefundene_zeilen = df[df[\"head\"].str.contains(\"Statistiken und Vergleich\", case=False, na=False)]\n",
    "\n",
    "# Ausgabe\n",
    "gefundene_zeilen"
   ],
   "id": "a9bc9361c93bbcbc"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Analyse von Dokumenten an Tagen mit geringerer Nachrichtenaktivität: bsp Sonntagen",
   "id": "9c798fa29fbdeb2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 16.03 nur 179 ansonsten um die 600\n",
    "# TODO Daten aus vortag noch nicht verfügbar\n",
    "subset_df = df[df['pubtime'].dt.date == pd.to_datetime('2025-04-01').date()]\n",
    "subset_df.shape\n"
   ],
   "id": "e9dafea24529c3a1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.manifold import TSNE\n",
    "import plotly.express as px\n",
    "import groq\n",
    "import re\n",
    "import json\n",
    "from collections import Counter\n",
    "import nltk\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "\n",
    "load_dotenv(dotenv_path='../.env')\n",
    "\n",
    "# Ensure that you have downloaded the punkt tokenizer for sentence splitting\n",
    "nltk.download('punkt')\n",
    "\n",
    "groq_key = os.getenv(\"GROQ_API_KEY\")\n",
    "# Initialize Groq client\n",
    "client = groq.Groq(api_key=groq_key)\n",
    "\n",
    "def split_text_sentencewise(text, max_length=1000):\n",
    "    \"\"\"Split the text into sentence-wise chunks that do not exceed max_length\"\"\"\n",
    "    sentences = nltk.sent_tokenize(text)  # Tokenize into sentences\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_length = 0\n",
    "\n",
    "    for sentence in sentences:\n",
    "        sentence_length = len(sentence.split())  # Count words (approximate tokens)\n",
    "\n",
    "        # If adding this sentence exceeds the max length, start a new chunk\n",
    "        if current_length + sentence_length > max_length:\n",
    "            if current_chunk:\n",
    "                chunks.append(' '.join(current_chunk))\n",
    "            current_chunk = [sentence]\n",
    "            current_length = sentence_length\n",
    "        else:\n",
    "            current_chunk.append(sentence)\n",
    "            current_length += sentence_length\n",
    "\n",
    "    # Add the last chunk if any\n",
    "    if current_chunk:\n",
    "        chunks.append(' '.join(current_chunk))\n",
    "\n",
    "    return chunks\n",
    "\n",
    "def parse_json_response(response):\n",
    "    \"\"\"Extract JSON from Groq response\"\"\"\n",
    "    try:\n",
    "        json_str = re.search(r'\\{.*\\}', response, re.DOTALL)\n",
    "        if json_str:\n",
    "            return json.loads(json_str.group())\n",
    "        return {}\n",
    "    except json.JSONDecodeError:\n",
    "        return {}\n",
    "\n",
    "def get_wikipedia_suggestions(text):\n",
    "    \"\"\"Get Wikipedia suggestions from Groq for a single text, ensuring the titles exist on Wikipedia\"\"\"\n",
    "\n",
    "    # Split the text into chunks sentence-wise to avoid exceeding LLM input size limit\n",
    "    chunks = split_text_sentencewise(text)\n",
    "\n",
    "    all_titles = []\n",
    "\n",
    "    for chunk in chunks:\n",
    "        prompt = (\n",
    "            \"Gib 3-5 relevante Wikipedia-Artikel-Titel für diesen Text als JSON. \"\n",
    "            \"Die Titel müssen echte Wikipedia-Artikel sein, d.h. sie müssen genau übereinstimmen. \"\n",
    "            \"Format: {'titles': ['Artikel1', 'Artikel2']}. Nur exakte Artikelnamen, keine Vermutungen oder Platzhalter:\\n\\n\"\n",
    "            f\"{chunk}\"  # Each chunk of the text\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            completion = client.chat.completions.create(\n",
    "                model=\"llama3-70b-8192\",\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"Du bist ein hilfreicher Assistent für Wikipedia-Recherche.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ],\n",
    "                temperature=0.5,\n",
    "                max_tokens=200,\n",
    "                response_format={\"type\": \"json_object\"}\n",
    "            )\n",
    "            response = completion.choices[0].message.content\n",
    "            titles = parse_json_response(response).get('titles', [])\n",
    "            all_titles.extend(titles)\n",
    "        except Exception as e:\n",
    "            print(f\"API Fehler bei Chunk: {str(e)}\")\n",
    "\n",
    "    return all_titles\n",
    "\n",
    "def get_common_suggestions(cluster_texts):\n",
    "    \"\"\"Analyze multiple texts to find common Wikipedia articles\"\"\"\n",
    "    all_articles = []\n",
    "\n",
    "    for text in cluster_texts:\n",
    "        articles = get_wikipedia_suggestions(text)\n",
    "        if articles:\n",
    "            all_articles.extend(articles)\n",
    "\n",
    "    if not all_articles:\n",
    "        return \"Keine relevanten Artikel gefunden\"\n",
    "\n",
    "    # Count and filter articles\n",
    "    counter = Counter(all_articles)\n",
    "    min_occurrences = max(2, len(cluster_texts) // 3)\n",
    "    common_articles = [\n",
    "        f\"{art} ({cnt}x)\"\n",
    "        for art, cnt in counter.most_common()\n",
    "        if cnt >= min_occurrences\n",
    "    ]\n",
    "\n",
    "    return \"\\n\".join(common_articles[:5]) if common_articles else \"Keine konsistenten Artikel\""
   ],
   "id": "f6018bfda22e0554"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def df_plot_dbscan(df, target_clusters=(4, 6)):\n",
    "    # Create a copy of the DataFrame to avoid modifying the original\n",
    "    working_df = df.copy()\n",
    "\n",
    "    if len(working_df) <= 400:\n",
    "        eps = 0.05\n",
    "        min_samples = 4\n",
    "    else:\n",
    "        eps = 0.04\n",
    "        min_samples = 6\n",
    "\n",
    "    # Add combined_text column\n",
    "    working_df[\"combined_text\"] = working_df[\"head\"] + \" \" + working_df[\"content\"]\n",
    "\n",
    "    model = SentenceTransformer('all-MiniLM-L12-v2')\n",
    "    embeddings = model.encode(working_df['combined_text'].tolist())\n",
    "\n",
    "    dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "    working_df['dbscan_cluster'] = dbscan.fit_predict(embeddings)\n",
    "\n",
    "    num_clusters = len(working_df['dbscan_cluster'].unique()) - (1 if -1 in working_df['dbscan_cluster'].unique() else 0)\n",
    "\n",
    "    while num_clusters < target_clusters[0] or num_clusters > target_clusters[1]:\n",
    "        if num_clusters < target_clusters[0]:\n",
    "            eps += 0.025\n",
    "        elif num_clusters > target_clusters[1]:\n",
    "            eps -= 0.025\n",
    "\n",
    "        dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "        working_df['dbscan_cluster'] = dbscan.fit_predict(embeddings)\n",
    "        num_clusters = len(working_df['dbscan_cluster'].unique()) - (1 if -1 in working_df['dbscan_cluster'].unique() else 0)\n",
    "\n",
    "    tsne = TSNE(n_components=2, random_state=42)\n",
    "    tsne_results = tsne.fit_transform(embeddings)\n",
    "    working_df['tsne_x'] = tsne_results[:, 0]\n",
    "    working_df['tsne_y'] = tsne_results[:, 1]\n",
    "\n",
    "    filtered_df = working_df[working_df[\"dbscan_cluster\"] >= 0]\n",
    "\n",
    "    fig_dbscan = px.scatter(filtered_df, x='tsne_x', y='tsne_y', color='dbscan_cluster', hover_data=['head'])\n",
    "\n",
    "    for cluster_id in sorted(filtered_df[\"dbscan_cluster\"].unique()):\n",
    "        cluster_data = filtered_df[filtered_df['dbscan_cluster'] == cluster_id]\n",
    "        print(f\"Cluster {cluster_id} heads:\")\n",
    "        for title in cluster_data['head'].tolist():\n",
    "            print(\"   \" + title)\n",
    "\n",
    "    fig_dbscan.show()\n",
    "\n",
    "    # Analyze each cluster\n",
    "    for cluster_id in sorted(filtered_df[\"dbscan_cluster\"].unique()):\n",
    "        cluster_data = filtered_df[filtered_df[\"dbscan_cluster\"] == cluster_id]\n",
    "        print(f\"\\nCluster {cluster_id} (Size: {len(cluster_data)})\")\n",
    "\n",
    "        # Get common Wikipedia articles\n",
    "        suggestions = get_common_suggestions(\n",
    "            cluster_data[\"combined_text\"].str[:1000].tolist()\n",
    "        )\n",
    "        print(\"\\nHäufigste Wikipedia-Artikel:\")\n",
    "        print(suggestions)"
   ],
   "id": "85610881c8ad9ac2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Process by date\n",
    "if 'pubtime' in df.columns:\n",
    "    for date in df['pubtime'].dt.date.unique():\n",
    "        date_df = df[df['pubtime'].dt.date == date]\n",
    "        print(f\"\\n\\n=== Themen für {date} ===\")\n",
    "        df_plot_dbscan(date_df)\n",
    "else:\n",
    "    print(\"\\n=== Analyse des gesamten Datensatzes ===\")\n",
    "    df_plot_dbscan(df)"
   ],
   "id": "de9c162ca5908065"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.cluster import DBSCAN, KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import groq\n",
    "import re\n",
    "import json\n",
    "from collections import Counter\n",
    "import nltk\n",
    "import os\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv(dotenv_path='../.env')\n",
    "\n",
    "# Ensure NLTK resources are downloaded\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Initialize Groq client\n",
    "groq_key = os.getenv(\"GROQ_API_KEY\")\n",
    "client = groq.Groq(api_key=groq_key)\n",
    "\n",
    "def split_text_sentencewise(text, max_length=1000):\n",
    "    \"\"\"Split the text into sentence-wise chunks that do not exceed max_length\"\"\"\n",
    "    sentences = nltk.sent_tokenize(text)  # Tokenize into sentences\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_length = 0\n",
    "\n",
    "    for sentence in sentences:\n",
    "        sentence_length = len(sentence.split())  # Count words (approximate tokens)\n",
    "\n",
    "        # If adding this sentence exceeds the max length, start a new chunk\n",
    "        if current_length + sentence_length > max_length:\n",
    "            if current_chunk:\n",
    "                chunks.append(' '.join(current_chunk))\n",
    "            current_chunk = [sentence]\n",
    "            current_length = sentence_length\n",
    "        else:\n",
    "            current_chunk.append(sentence)\n",
    "            current_length += sentence_length\n",
    "\n",
    "    # Add the last chunk if any\n",
    "    if current_chunk:\n",
    "        chunks.append(' '.join(current_chunk))\n",
    "\n",
    "    return chunks\n",
    "\n",
    "def parse_json_response(response):\n",
    "    \"\"\"Extract JSON from Groq response\"\"\"\n",
    "    try:\n",
    "        json_str = re.search(r'\\{.*\\}', response, re.DOTALL)\n",
    "        if json_str:\n",
    "            return json.loads(json_str.group())\n",
    "        return {}\n",
    "    except json.JSONDecodeError:\n",
    "        return {}\n",
    "\n",
    "def extract_keywords_from_text(text, top_n=15):\n",
    "    \"\"\"Extract key terms from text using TF-IDF\"\"\"\n",
    "    # Define German stopwords\n",
    "    german_stopwords = set([\n",
    "        'der', 'die', 'das', 'den', 'dem', 'des', 'ein', 'eine', 'einen', 'einem',\n",
    "        'eines', 'und', 'oder', 'aber', 'auch', 'als', 'zu', 'bei', 'mit', 'von',\n",
    "        'für', 'ist', 'sind', 'war', 'wird', 'werden', 'wurde', 'wurden', 'dass',\n",
    "        'daß', 'hat', 'haben', 'hatte', 'hätte', 'auf', 'aus', 'nach', 'über', 'unter',\n",
    "        'vor', 'in', 'an', 'am', 'um', 'durch', 'gegen', 'so', 'da', 'wie', 'wo',\n",
    "        'wann', 'was', 'wer', 'warum', 'wieso', 'welche', 'welcher', 'welches'\n",
    "    ])\n",
    "\n",
    "    # Create a TF-IDF vectorizer\n",
    "    vectorizer = TfidfVectorizer(\n",
    "        min_df=1, max_df=0.95,\n",
    "        stop_words=german_stopwords,\n",
    "        ngram_range=(1, 2)  # Include single words and bigrams\n",
    "    )\n",
    "\n",
    "    # Fit to the text\n",
    "    try:\n",
    "        X = vectorizer.fit_transform([text])\n",
    "        feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "        # Get top terms\n",
    "        tfidf_scores = zip(feature_names, X.toarray()[0])\n",
    "        sorted_scores = sorted(tfidf_scores, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        return [term for term, score in sorted_scores[:top_n]]\n",
    "    except:\n",
    "        return []\n",
    "\n",
    "def get_wikipedia_suggestions(text):\n",
    "    \"\"\"Get Wikipedia suggestions from Groq for a single text, ensuring the titles exist on Wikipedia\"\"\"\n",
    "    # Extract keywords to help Groq focus on important terms\n",
    "    keywords = extract_keywords_from_text(text[:5000])\n",
    "    keywords_text = \", \".join(keywords[:10])\n",
    "\n",
    "    # Split the text into chunks\n",
    "    chunks = split_text_sentencewise(text[:3000])  # Limit to first 3000 chars for efficiency\n",
    "    all_titles = []\n",
    "\n",
    "    for chunk in chunks:\n",
    "        prompt = (\n",
    "            \"Identifiziere 3-5 relevante deutsche Wikipedia-Artikel für diesen Text. \"\n",
    "            \"Die Titel MÜSSEN exakt mit echten deutschsprachigen Wikipedia-Artikeln übereinstimmen. \"\n",
    "            f\"Wichtige Begriffe im Text: {keywords_text}\\n\\n\"\n",
    "            \"Format: {'titles': ['Artikeltitel1', 'Artikeltitel2', ...]}\\n\\n\"\n",
    "            f\"Text: {chunk}\"\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            completion = client.chat.completions.create(\n",
    "                model=\"llama3-70b-8192\",\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"Du bist ein präziser Assistent für die Identifikation von deutschen Wikipedia-Artikeln zu Nachrichtentexten. Du gibst nur exakte, existierende Artikeltitel zurück.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ],\n",
    "                temperature=0.3,\n",
    "                max_tokens=250,\n",
    "                response_format={\"type\": \"json_object\"}\n",
    "            )\n",
    "            response = completion.choices[0].message.content\n",
    "            titles = parse_json_response(response).get('titles', [])\n",
    "            all_titles.extend(titles)\n",
    "        except Exception as e:\n",
    "            print(f\"API Fehler bei Chunk: {str(e)}\")\n",
    "\n",
    "    return all_titles\n",
    "\n",
    "def get_common_suggestions(cluster_texts):\n",
    "    \"\"\"Analyze multiple texts to find common Wikipedia articles\"\"\"\n",
    "    all_articles = []\n",
    "\n",
    "    # Limit to 3 texts per cluster for efficiency\n",
    "    for text in cluster_texts[:3]:\n",
    "        articles = get_wikipedia_suggestions(text)\n",
    "        if articles:\n",
    "            all_articles.extend(articles)\n",
    "\n",
    "    if not all_articles:\n",
    "        return \"Keine relevanten Artikel gefunden\"\n",
    "\n",
    "    # Count and filter articles\n",
    "    counter = Counter(all_articles)\n",
    "    # Lower threshold for smaller datasets\n",
    "    min_occurrences = 1\n",
    "    common_articles = [\n",
    "        f\"{art} ({cnt}x)\"\n",
    "        for art, cnt in counter.most_common(8)\n",
    "        if cnt >= min_occurrences\n",
    "    ]\n",
    "\n",
    "    return \"\\n\".join(common_articles[:7]) if common_articles else \"Keine konsistenten Artikel\"\n",
    "\n",
    "def extract_cluster_key_terms(texts, n_terms=8):\n",
    "    \"\"\"Extract key terms that characterize a cluster using TF-IDF\"\"\"\n",
    "    # German stopwords\n",
    "    german_stopwords = set([\n",
    "        'der', 'die', 'das', 'den', 'dem', 'des', 'ein', 'eine', 'einen', 'einem',\n",
    "        'eines', 'und', 'oder', 'aber', 'auch', 'als', 'zu', 'bei', 'mit', 'von',\n",
    "        'für', 'ist', 'sind', 'war', 'wird', 'werden', 'wurde', 'wurden', 'dass',\n",
    "        'daß', 'hat', 'haben', 'hatte', 'hätte', 'auf', 'aus', 'nach', 'über', 'unter',\n",
    "        'vor', 'in', 'an', 'am', 'um', 'durch', 'gegen', 'so', 'da', 'wie', 'wo',\n",
    "        'wann', 'was', 'wer', 'warum', 'wieso', 'welche', 'welcher', 'welches',\n",
    "        'mehr', 'noch', 'sehr', 'schon', 'wieder', 'immer', 'nur', 'etwa', 'bereits'\n",
    "    ])\n",
    "\n",
    "    # Create a TF-IDF vectorizer specific for German news\n",
    "    vectorizer = TfidfVectorizer(\n",
    "        min_df=1, max_df=0.9,  # More lenient to work with smaller clusters\n",
    "        stop_words=list(german_stopwords),\n",
    "        ngram_range=(1, 2)  # Include both single words and bigrams\n",
    "    )\n",
    "\n",
    "    # If not enough texts, return empty\n",
    "    if len(texts) < 1:\n",
    "        return \"Nicht genügend Artikel für Schlüsselwortextraktion\"\n",
    "\n",
    "    try:\n",
    "        # Fit the vectorizer to all texts in the cluster\n",
    "        X = vectorizer.fit_transform(texts)\n",
    "        feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "        # Calculate average TF-IDF for each term\n",
    "        tfidf_means = X.mean(axis=0).A1\n",
    "\n",
    "        # Get top terms by mean TF-IDF score\n",
    "        top_indices = tfidf_means.argsort()[-n_terms:][::-1]\n",
    "        top_terms = [feature_names[i] for i in top_indices]\n",
    "\n",
    "        return \", \".join(top_terms)\n",
    "    except Exception as e:\n",
    "        print(f\"Fehler bei Schlüsselwortextraktion: {str(e)}\")\n",
    "        return \"Fehler bei der Extraktion von Schlüsselwörtern\"\n",
    "\n",
    "def get_cluster_summary(texts, max_length=150):\n",
    "    \"\"\"Get a summary of the cluster using Groq\"\"\"\n",
    "    # Combine the first sentences of each article for context\n",
    "    first_sentences = []\n",
    "    for text in texts[:3]:  # Use up to 3 texts\n",
    "        sentences = nltk.sent_tokenize(text)\n",
    "        if sentences:\n",
    "            first_sentences.append(sentences[0])\n",
    "\n",
    "    context = \" \".join(first_sentences)\n",
    "\n",
    "    # Extract keywords to guide summary\n",
    "    keywords = extract_keywords_from_text(\" \".join(texts), top_n=10)\n",
    "\n",
    "    prompt = (\n",
    "        \"Fasse den Hauptnachrichtentrend dieser Artikelgruppe in 1-2 Sätzen zusammen \"\n",
    "        \"(maximal 150 Zeichen). Benutze einen neutralen, nachrichtlichen Stil:\\n\\n\"\n",
    "        f\"Schlüsselwörter: {', '.join(keywords)}\\n\\n\"\n",
    "        f\"Kontext: {context[:1000]}\"\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        completion = client.chat.completions.create(\n",
    "            model=\"llama3-70b-8192\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"Du bist ein präziser Nachrichtenredakteur. Fasse den Haupttrend in 1-2 kurzen, prägnanten Sätzen zusammen.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            temperature=0.3,\n",
    "            max_tokens=100\n",
    "        )\n",
    "        summary = completion.choices[0].message.content\n",
    "        # Truncate if too long\n",
    "        if len(summary) > max_length:\n",
    "            summary = summary[:max_length] + \"...\"\n",
    "        return summary\n",
    "    except Exception as e:\n",
    "        print(f\"Fehler bei Zusammenfassung: {str(e)}\")\n",
    "        return \"Zusammenfassung nicht verfügbar\"\n",
    "\n",
    "def df_cluster_news(df, n_clusters=None):\n",
    "    \"\"\"Create clusters using KMeans and visualize them\"\"\"\n",
    "    # Create a copy of the DataFrame\n",
    "    working_df = df.copy()\n",
    "\n",
    "    # Set number of clusters based on dataset size\n",
    "    if n_clusters is None:\n",
    "        if len(working_df) <= 50:\n",
    "            n_clusters = 3\n",
    "        elif len(working_df) <= 200:\n",
    "            n_clusters = 5\n",
    "        elif len(working_df) <= 500:\n",
    "            n_clusters = 6\n",
    "        else:\n",
    "            n_clusters = min(8, len(working_df) // 200 + 5)  # Scale with dataset size\n",
    "\n",
    "    print(f\"Clustering {len(working_df)} Artikel in {n_clusters} Cluster...\")\n",
    "\n",
    "    # Weight headlines more heavily in the combined text\n",
    "    working_df[\"combined_text\"] = working_df[\"head\"].str.repeat(3) + \" \" + working_df[\"content\"]\n",
    "\n",
    "    # Use a better model for German text\n",
    "    model = SentenceTransformer('paraphrase-multilingual-mpnet-base-v2')\n",
    "    print(\"Erstelle Embeddings...\")\n",
    "    embeddings = model.encode(working_df['combined_text'].tolist(), show_progress_bar=True)\n",
    "\n",
    "    # Apply KMeans clustering\n",
    "    print(f\"Wende KMeans mit {n_clusters} Clustern an...\")\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
    "    working_df['cluster'] = kmeans.fit_predict(embeddings)\n",
    "\n",
    "    # Reduce dimensionality for visualization\n",
    "    print(\"Reduziere Dimensionen für Visualisierung...\")\n",
    "    # First reduce to 50 dimensions with PCA for efficiency\n",
    "    if len(working_df) > 100:\n",
    "        pca = PCA(n_components=min(50, len(working_df)-1))\n",
    "        reduced_data = pca.fit_transform(embeddings)\n",
    "    else:\n",
    "        reduced_data = embeddings\n",
    "\n",
    "    # Then apply t-SNE\n",
    "    perplexity_value = min(30, max(5, len(working_df) // 10))\n",
    "    tsne = TSNE(n_components=2, random_state=42, perplexity=perplexity_value)\n",
    "    tsne_results = tsne.fit_transform(reduced_data)\n",
    "\n",
    "    working_df['tsne_x'] = tsne_results[:, 0]\n",
    "    working_df['tsne_y'] = tsne_results[:, 1]\n",
    "\n",
    "    # Create a more visually appealing scatter plot\n",
    "    fig = px.scatter(\n",
    "        working_df,\n",
    "        x='tsne_x',\n",
    "        y='tsne_y',\n",
    "        color='cluster',\n",
    "        color_continuous_scale=px.colors.qualitative.Bold,\n",
    "        hover_data=['head'],\n",
    "        title=f\"Themen-Clustering ({n_clusters} Cluster)\"\n",
    "    )\n",
    "\n",
    "    # Enhance the visualization\n",
    "    fig.update_traces(marker=dict(size=10, opacity=0.7))\n",
    "    fig.update_layout(\n",
    "        legend_title_text='Cluster',\n",
    "        xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),\n",
    "        yaxis=dict(showgrid=False, zeroline=False, showticklabels=False)\n",
    "    )\n",
    "\n",
    "    # Add annotations for cluster centers\n",
    "    cluster_centers = []\n",
    "    for cluster_id in range(n_clusters):\n",
    "        cluster_df = working_df[working_df['cluster'] == cluster_id]\n",
    "        center_x = cluster_df['tsne_x'].mean()\n",
    "        center_y = cluster_df['tsne_y'].mean()\n",
    "        cluster_centers.append((center_x, center_y))\n",
    "\n",
    "        # Add a text label at the cluster center\n",
    "        fig.add_annotation(\n",
    "            x=center_x, y=center_y,\n",
    "            text=f\"Cluster {cluster_id}\",\n",
    "            showarrow=False,\n",
    "            font=dict(size=14, color=\"black\", family=\"Arial Black\"),\n",
    "            bgcolor=\"rgba(255, 255, 255, 0.7)\",\n",
    "            bordercolor=\"black\",\n",
    "            borderwidth=1\n",
    "        )\n",
    "\n",
    "    # Sort clusters by size (descending)\n",
    "    clusters_by_size = working_df['cluster'].value_counts().sort_values(ascending=False).index\n",
    "\n",
    "    # Process each cluster\n",
    "    for cluster_id in clusters_by_size:\n",
    "        cluster_data = working_df[working_df['cluster'] == cluster_id]\n",
    "        cluster_size = len(cluster_data)\n",
    "\n",
    "        print(f\"\\n=== Cluster {cluster_id} (Größe: {cluster_size}) ===\")\n",
    "\n",
    "        # Extract key terms\n",
    "        key_terms = extract_cluster_key_terms(cluster_data[\"combined_text\"].tolist())\n",
    "        print(f\"Schlüsselwörter: {key_terms}\")\n",
    "\n",
    "        # Get cluster summary\n",
    "        if cluster_size >= 2:\n",
    "            summary = get_cluster_summary(cluster_data[\"combined_text\"].tolist())\n",
    "            print(f\"Zusammenfassung: {summary}\")\n",
    "\n",
    "        # Show representative headlines\n",
    "        print(\"\\nSchlagzeilen:\")\n",
    "        for title in cluster_data['head'].tolist()[:5]:  # Show top 5 headlines\n",
    "            print(f\"• {title}\")\n",
    "\n",
    "        # Get common Wikipedia articles\n",
    "        print(\"\\nWikipedia-Artikel:\")\n",
    "        suggestions = get_common_suggestions(\n",
    "            cluster_data[\"combined_text\"].str[:1500].tolist()\n",
    "        )\n",
    "        print(suggestions)\n",
    "\n",
    "    # Show plot\n",
    "    fig.show()\n",
    "\n",
    "    return working_df\n",
    "\n",
    "# Function to process by date\n",
    "def process_by_date(df):\n",
    "    if 'pubtime' in df.columns:\n",
    "        for date in sorted(df['pubtime'].dt.date.unique()):\n",
    "            date_df = df[df['pubtime'].dt.date == date]\n",
    "            if len(date_df) > 0:\n",
    "                print(f\"\\n\\n=== Themen für {date} ===\")\n",
    "                df_cluster_news(date_df)\n",
    "            else:\n",
    "                print(f\"Keine Artikel für {date} verfügbar.\")\n",
    "    else:\n",
    "        print(\"\\n=== Analyse des gesamten Datensatzes ===\")\n",
    "        df_cluster_news(df)\n",
    "process_by_date(df)"
   ],
   "id": "eacf5d91febcba55"
  }
 ],
 "metadata": {},
 "nbformat": 5,
 "nbformat_minor": 9
}
