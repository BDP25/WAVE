{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_parquet(\"cleaned_data/cleaned_data.parquet\", engine=\"pyarrow\")"
   ],
   "id": "57bc3f0fa68b51bb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.manifold import TSNE\n",
    "import plotly.express as px\n",
    "import groq\n",
    "import re\n",
    "import json\n",
    "from collections import Counter\n",
    "import nltk\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "\n",
    "load_dotenv(dotenv_path='../.env')\n",
    "\n",
    "# Ensure that you have downloaded the punkt tokenizer for sentence splitting\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "\n",
    "\n",
    "groq_key = os.getenv(\"GROQ_API_KEY\")\n",
    "# Initialize Groq client\n",
    "client = groq.Groq(api_key=groq_key)\n",
    "\n",
    "def split_text_sentencewise(text, max_length=1000):\n",
    "    \"\"\"Split the text into sentence-wise chunks that do not exceed max_length\"\"\"\n",
    "    sentences = nltk.sent_tokenize(text)  # Tokenize into sentences\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_length = 0\n",
    "\n",
    "    for sentence in sentences:\n",
    "        sentence_length = len(sentence.split())  # Count words (approximate tokens)\n",
    "\n",
    "        # If adding this sentence exceeds the max length, start a new chunk\n",
    "        if current_length + sentence_length > max_length:\n",
    "            if current_chunk:\n",
    "                chunks.append(' '.join(current_chunk))\n",
    "            current_chunk = [sentence]\n",
    "            current_length = sentence_length\n",
    "        else:\n",
    "            current_chunk.append(sentence)\n",
    "            current_length += sentence_length\n",
    "\n",
    "    # Add the last chunk if any\n",
    "    if current_chunk:\n",
    "        chunks.append(' '.join(current_chunk))\n",
    "\n",
    "    return chunks\n",
    "\n",
    "def parse_json_response(response):\n",
    "    \"\"\"Extract JSON from Groq response\"\"\"\n",
    "    try:\n",
    "        json_str = re.search(r'\\{.*\\}', response, re.DOTALL)\n",
    "        if json_str:\n",
    "            return json.loads(json_str.group())\n",
    "        return {}\n",
    "    except json.JSONDecodeError:\n",
    "        return {}\n",
    "\n",
    "def get_wikipedia_suggestions(text):\n",
    "    \"\"\"Get Wikipedia suggestions from Groq for a single text, ensuring the titles exist on Wikipedia\"\"\"\n",
    "\n",
    "    # Split the text into chunks sentence-wise to avoid exceeding LLM input size limit\n",
    "    chunks = split_text_sentencewise(text)\n",
    "\n",
    "    all_titles = []\n",
    "\n",
    "    for chunk in chunks:\n",
    "        prompt = (\n",
    "            \"Gib 3-5 relevante Wikipedia-Artikel-Titel für diesen Text als JSON. \"\n",
    "            \"Die Titel müssen echte Wikipedia-Artikel sein, d.h. sie müssen genau übereinstimmen. \"\n",
    "            \"Format: {'titles': ['Artikel1', 'Artikel2']}. Nur exakte Artikelnamen, keine Vermutungen oder Platzhalter:\\n\\n\"\n",
    "            f\"{chunk}\"  # Each chunk of the text\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            completion = client.chat.completions.create(\n",
    "                model=\"llama3-70b-8192\",\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"Du bist ein hilfreicher Assistent für Wikipedia-Recherche.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ],\n",
    "                temperature=0.5,\n",
    "                max_tokens=200,\n",
    "                response_format={\"type\": \"json_object\"}\n",
    "            )\n",
    "            response = completion.choices[0].message.content\n",
    "            titles = parse_json_response(response).get('titles', [])\n",
    "            all_titles.extend(titles)\n",
    "        except Exception as e:\n",
    "            print(f\"API Fehler bei Chunk: {str(e)}\")\n",
    "\n",
    "    return all_titles\n",
    "\n",
    "def get_common_suggestions(cluster_texts):\n",
    "    \"\"\"Analyze multiple texts to find common Wikipedia articles\"\"\"\n",
    "    all_articles = []\n",
    "\n",
    "    for text in cluster_texts:\n",
    "        articles = get_wikipedia_suggestions(text)\n",
    "        if articles:\n",
    "            all_articles.extend(articles)\n",
    "\n",
    "    if not all_articles:\n",
    "        return \"Keine relevanten Artikel gefunden\"\n",
    "\n",
    "    # Count and filter articles\n",
    "    counter = Counter(all_articles)\n",
    "    min_occurrences = max(2, len(cluster_texts) // 3)\n",
    "    common_articles = [\n",
    "        f\"{art} ({cnt}x)\"\n",
    "        for art, cnt in counter.most_common()\n",
    "        if cnt >= min_occurrences\n",
    "    ]\n",
    "\n",
    "    return \"\\n\".join(common_articles[:5]) if common_articles else \"Keine konsistenten Artikel\"\n"
   ],
   "id": "76789bbe391893f6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import matplotlib.cm as cm\n",
    "def plot_silhouette_analysis(X, cluster_labels, n_clusters):\n",
    "    \"\"\"Create silhouette plot for cluster evaluation\"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "    fig.set_size_inches(18, 7)\n",
    "\n",
    "    # Silhouette plot\n",
    "    ax1.set_xlim([-0.1, 1])\n",
    "    ax1.set_ylim([0, len(X) + (n_clusters + 1) * 10])\n",
    "\n",
    "    silhouette_avg = silhouette_score(X, cluster_labels)\n",
    "    print(f\"Silhouette Score: {silhouette_avg:.3f}\")\n",
    "\n",
    "    sample_silhouette_values = silhouette_samples(X, cluster_labels)\n",
    "\n",
    "    y_lower = 10\n",
    "    for i in range(n_clusters):\n",
    "        ith_cluster_silhouette_values = sample_silhouette_values[cluster_labels == i]\n",
    "        ith_cluster_silhouette_values.sort()\n",
    "\n",
    "        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
    "        y_upper = y_lower + size_cluster_i\n",
    "\n",
    "        color = cm.nipy_spectral(float(i) / n_clusters)\n",
    "        ax1.fill_betweenx(\n",
    "            np.arange(y_lower, y_upper),\n",
    "            0,\n",
    "            ith_cluster_silhouette_values,\n",
    "            facecolor=color,\n",
    "            edgecolor=color,\n",
    "            alpha=0.7,\n",
    "        )\n",
    "\n",
    "        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
    "        y_lower = y_upper + 10\n",
    "\n",
    "    ax1.set_title(\"Silhouette Plot\")\n",
    "    ax1.set_xlabel(\"Silhouette Coefficient\")\n",
    "    ax1.set_ylabel(\"Cluster Label\")\n",
    "    ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
    "    ax1.set_yticks([])\n",
    "    ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n",
    "\n",
    "    # Cluster visualization\n",
    "    tsne = TSNE(n_components=2, random_state=42)\n",
    "    X_tsne = tsne.fit_transform(X)\n",
    "\n",
    "    colors = cm.nipy_spectral(cluster_labels.astype(float) / n_clusters)\n",
    "    ax2.scatter(\n",
    "        X_tsne[:, 0], X_tsne[:, 1],\n",
    "        marker='.', s=30, lw=0, alpha=0.7,\n",
    "        c=colors, edgecolor='k'\n",
    "    )\n",
    "\n",
    "    ax2.set_title(\"Cluster Visualization\")\n",
    "    ax2.set_xlabel(\"t-SNE 1\")\n",
    "    ax2.set_ylabel(\"t-SNE 2\")\n",
    "\n",
    "    plt.suptitle(f\"Silhouette Analysis for DBSCAN Clustering (n_clusters={n_clusters})\",\n",
    "                fontsize=14, fontweight='bold')\n",
    "    plt.show()\n",
    "\n",
    "def df_plot_dbscan(df, target_clusters=(4, 6)):\n",
    "    if len(df) <= 400:\n",
    "        eps = 0.05\n",
    "        min_samples = 4\n",
    "    else:\n",
    "        eps = 0.04\n",
    "        min_samples = 6\n",
    "\n",
    "    df.loc[:, \"combined_text\"] = df[\"head\"] + \" \" + df[\"content\"]\n",
    "\n",
    "    model = SentenceTransformer('all-MiniLM-L12-v2')\n",
    "    embeddings = model.encode(df['combined_text'].tolist())\n",
    "\n",
    "    dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "    df.loc[:, 'dbscan_cluster'] = dbscan.fit_predict(embeddings)\n",
    "\n",
    "    num_clusters = len(df['dbscan_cluster'].unique()) - (1 if -1 in df['dbscan_cluster'].unique() else 0)\n",
    "\n",
    "    while num_clusters < target_clusters[0] or num_clusters > target_clusters[1]:\n",
    "        if num_clusters < target_clusters[0]:\n",
    "            eps += 0.025\n",
    "        elif num_clusters > target_clusters[1]:\n",
    "            eps -= 0.025\n",
    "\n",
    "        dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "        df.loc[:, 'dbscan_cluster'] = dbscan.fit_predict(embeddings)\n",
    "        num_clusters = len(df['dbscan_cluster'].unique()) - (1 if -1 in df['dbscan_cluster'].unique() else 0)\n",
    "\n",
    "    tsne = TSNE(n_components=2, random_state=42)\n",
    "    tsne_results = tsne.fit_transform(embeddings)\n",
    "    df.loc[:, 'tsne_x'] = tsne_results[:, 0]\n",
    "    df.loc[:, 'tsne_y'] = tsne_results[:, 1]\n",
    "\n",
    "    filtered_df = df[df[\"dbscan_cluster\"] >= 0]\n",
    "    n_clusters = len(filtered_df['dbscan_cluster'].unique())\n",
    "\n",
    "    # Plot silhouette analysis\n",
    "    if n_clusters > 1:  # Silhouette score requires at least 2 clusters\n",
    "        plot_silhouette_analysis(\n",
    "            embeddings[df['dbscan_cluster'] >= 0],\n",
    "            filtered_df['dbscan_cluster'].values,\n",
    "            n_clusters\n",
    "        )\n",
    "\n",
    "    fig_dbscan = px.scatter(filtered_df, x='tsne_x', y='tsne_y', color='dbscan_cluster', hover_data=['head'])\n",
    "\n",
    "    for cluster_id in sorted(filtered_df[\"dbscan_cluster\"].unique()):\n",
    "        cluster_data = filtered_df[filtered_df['dbscan_cluster'] == cluster_id]\n",
    "        print(f\"Cluster {cluster_id} heads:\")\n",
    "        for title in cluster_data['head'].tolist():\n",
    "            print(\"   \" + title)\n",
    "\n",
    "    fig_dbscan.show()\n",
    "\n",
    "    # Analyze each cluster\n",
    "    for cluster_id in sorted(filtered_df[\"dbscan_cluster\"].unique()):\n",
    "        cluster_data = filtered_df[filtered_df[\"dbscan_cluster\"] == cluster_id]\n",
    "        print(f\"\\nCluster {cluster_id} (Size: {len(cluster_data)})\")\n",
    "\n",
    "        # Get common Wikipedia articles\n",
    "        suggestions = get_common_suggestions(\n",
    "            cluster_data[\"combined_text\"].str[:1000].tolist()\n",
    "        )\n",
    "        print(\"\\nHäufigste Wikipedia-Artikel:\")\n",
    "        print(suggestions)\n",
    "\n",
    "# Process by date\n",
    "if 'pubtime' in df.columns:\n",
    "    for date in df['pubtime'].dt.date.unique():\n",
    "        date_df = df[df['pubtime'].dt.date == date]\n",
    "        print(f\"\\n\\n=== Themen für {date} ===\")\n",
    "        df_plot_dbscan(date_df)\n",
    "else:\n",
    "    print(\"\\n=== Analyse des gesamten Datensatzes ===\")\n",
    "    df_plot_dbscan(df)"
   ],
   "id": "fe7abd2156605b93"
  }
 ],
 "metadata": {},
 "nbformat": 5,
 "nbformat_minor": 9
}
