{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Data Cleaning",
   "id": "1949043c19ea3403"
  },
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-03-26T10:30:10.816882Z",
     "start_time": "2025-03-26T10:26:16.409806Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import re\n",
    "from io import open\n",
    "\n",
    "# Load the file\n",
    "file_path = '7e97a6db-e083-410c-b84d-05240030faa9__2025_03_22T19_19_40.tsv.xz'\n",
    "df = pd.read_csv(file_path, sep='\\t', compression='xz')\n",
    "\n",
    "# Clean column names (remove extra spaces)\n",
    "df.columns = df.columns.str.strip()\n",
    "\n",
    "# Function to clean text by removing HTML tags, URLs, and extra spaces\n",
    "def clean_text(text):\n",
    "    if pd.isna(text):  # Return empty string if value is NaN\n",
    "        return ''\n",
    "    text = re.sub(r'<[^>]+>', ' ', text)  # Remove HTML tags\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', ' ', text)  # Remove URLs\n",
    "    text = re.sub(r'&[a-zA-Z0-9#]+;', ' ', text)  # Remove HTML entities\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()  # Reduce multiple spaces\n",
    "    return text\n",
    "\n",
    "def remove_numbered_links(df):\n",
    "    # Create a dictionary to store the base links and their corresponding rows\n",
    "    link_dict = {}\n",
    "    nan_rows = []\n",
    "\n",
    "    # Iterate over the DataFrame rows\n",
    "    for index, row in df.iterrows():\n",
    "        # If the article_link is NaN, add the row index to nan_rows\n",
    "        if pd.isna(row['article_link']):\n",
    "            nan_rows.append(index)\n",
    "            continue\n",
    "\n",
    "        # Extract the base link by removing the trailing '-number' if present at the end\n",
    "        base_link = re.sub(r'-\\d+$', '', row['article_link'])\n",
    "\n",
    "        # If the base link is not in the dictionary, add it\n",
    "        if base_link not in link_dict:\n",
    "            link_dict[base_link] = index\n",
    "        else:\n",
    "            # If the base link is already in the dictionary, compare the current row with the stored row\n",
    "            existing_index = link_dict[base_link]\n",
    "            if re.search(r'-\\d+$', df.at[existing_index, 'article_link']) and df.at[existing_index, 'head'] == row['head']:\n",
    "                # If the existing link has a '-number' suffix and the titles are the same, replace it with the current row\n",
    "                link_dict[base_link] = index\n",
    "\n",
    "    # Get the indices to keep, including rows with NaN article_link\n",
    "    indices_to_keep = list(link_dict.values()) + nan_rows\n",
    "\n",
    "    # Filter the DataFrame to keep only the rows with the desired indices\n",
    "    df = df.loc[indices_to_keep].reset_index(drop=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "def remove_similar_rows(df, threshold=0.995):\n",
    "    # Drop rows where the content_id is the same\n",
    "    df = df.drop_duplicates(subset=['content_id'])\n",
    "\n",
    "    # Clean the content column\n",
    "    df.loc[:, 'content'] = df['content'].apply(clean_text)\n",
    "\n",
    "    # Drop rows where the content is exactly the same\n",
    "    df = df.drop_duplicates(subset=['content'])\n",
    "\n",
    "    df = remove_numbered_links(df)\n",
    "\n",
    "    # Vectorize the content using TF-IDF\n",
    "    vectorizer = TfidfVectorizer().fit_transform(df['content'])\n",
    "    vectors = vectorizer.toarray()\n",
    "\n",
    "    # Compute cosine similarity matrix\n",
    "    cosine_sim_matrix = cosine_similarity(vectors)\n",
    "\n",
    "    # Identify pairs of articles with similarity above the threshold\n",
    "    similar_pairs = np.where(cosine_sim_matrix > threshold)\n",
    "\n",
    "    # Create a set of indices to drop\n",
    "    indices_to_drop = set()\n",
    "    for i, j in zip(*similar_pairs):\n",
    "        if i != j:\n",
    "            indices_to_drop.add(j)\n",
    "\n",
    "    # Drop the duplicates using .loc to avoid SettingWithCopyWarning\n",
    "    df = df.loc[~df.index.isin(indices_to_drop)]\n",
    "\n",
    "    # Reset index\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "# Drop same or nearly same articles\n",
    "df = remove_similar_rows(df, 0.65)\n",
    "\n",
    "with open('output_file.tsv', 'w') as f:\n",
    "    df.to_csv(f, sep='\\t', index=False)\n",
    "\n",
    "# make the pubtime a df datetime format\n",
    "df['pubtime'] = pd.to_datetime(df['pubtime'])\n",
    "\n",
    "df"
   ],
   "id": "initial_id",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "            id                   pubtime medium_code           medium_name  \\\n",
       "0     56411905 2025-03-20 10:25:38+01:00        NNTA      tagesanzeiger.ch   \n",
       "1     56360926 2025-03-14 17:44:58+01:00        NNBE      bernerzeitung.ch   \n",
       "2     56349050 2025-03-13 14:53:13+01:00         SRF                srf.ch   \n",
       "3     56370481 2025-03-15 23:35:40+01:00        NNBE      bernerzeitung.ch   \n",
       "4     56372428 2025-03-16 09:41:18+01:00         SRF                srf.ch   \n",
       "...        ...                       ...         ...                   ...   \n",
       "4443  56340876 2025-03-13 00:00:00+01:00         NZZ  Neue Zürcher Zeitung   \n",
       "4444  56329575 2025-03-12 00:00:00+01:00         BLI                 Blick   \n",
       "4445  56345547 2025-03-13 00:00:00+01:00         WEW         Die Weltwoche   \n",
       "4446  56395580 2025-03-19 00:00:00+01:00         OLT       Oltner Tagblatt   \n",
       "4447  56352301 2025-03-14 00:00:00+01:00         BLI                 Blick   \n",
       "\n",
       "      char_count                                               head  \\\n",
       "0           2406  Flughafen Zürich: Swiss-Flug nach Budapest mus...   \n",
       "1           1331    Zu hohe Temperaturen: Rennabbruch an der Bob-WM   \n",
       "2           2595  Mit den «Drachen» zum selben Höhenflug ansetze...   \n",
       "3           1861  Eishockey-Playoffs: Lausanne muss den Ausgleic...   \n",
       "4           3064  Norris' «stressiger» Sieg – Hülkenberg mit «Ba...   \n",
       "...          ...                                                ...   \n",
       "4443       12728          Die Labor-These gewinnt wieder an Gewicht   \n",
       "4444        2737             Lemieux – der Mann fürs Grobe beim HCD   \n",
       "4445        5286            Friedrich Merz und der Schulden-Tsunami   \n",
       "4446        3172                      «Wollen machen statt jammern»   \n",
       "4447        1840  Der letzte Schweizer Rad-Star der goldenen 50e...   \n",
       "\n",
       "                                           article_link  \\\n",
       "0     https://www.tagesanzeiger.ch/flughafen-zuerich...   \n",
       "1     https://www.bernerzeitung.ch/bob-wm-lake-placi...   \n",
       "2     https://www.srf.ch/sport/eishockey/national-le...   \n",
       "3     https://www.bernerzeitung.ch/eishockey-tigers-...   \n",
       "4     https://www.srf.ch/sport/motorsport/formel-1/s...   \n",
       "...                                                 ...   \n",
       "4443                                                NaN   \n",
       "4444                                                NaN   \n",
       "4445                                                NaN   \n",
       "4446                                                NaN   \n",
       "4447                                                NaN   \n",
       "\n",
       "                                content_id  \\\n",
       "0     0031313b-f8f4-5281-70d8-9727db8ef8f6   \n",
       "1     003aa8b4-f90d-63f2-636c-33a3b91f0aea   \n",
       "2     007555d5-6e16-cb17-b0d0-213bc53cb964   \n",
       "3     00a0ad13-aae5-160e-e51c-f7ddaa40841e   \n",
       "4     00e2d2b8-b04f-1963-5af5-60e5eeddf84f   \n",
       "...                                    ...   \n",
       "4443  ffbb940a-c766-37ed-6f9b-bded485bf53b   \n",
       "4444  ffcd9173-60dd-68ce-6603-36f63f76c0ca   \n",
       "4445  ffce0fee-1078-691a-2e49-dcbf33efb38e   \n",
       "4446  ffd09e01-c429-b69f-cc68-2f68952469bf   \n",
       "4447  ffe6a854-a7a2-1da9-fa81-577d7ca6c859   \n",
       "\n",
       "                                                content  \n",
       "0     Ein Flug der Swiss, durchgeführt von AirBaltic...  \n",
       "1     Die Sonne sorgt an der Bob-WM für schwierige V...  \n",
       "2     2016 wurde Leuenberger mit dem SC Bern als Int...  \n",
       "3     Lausanne verliert im zweiten Playoff-Viertelfi...  \n",
       "4     Erleichterung bei Sauber nach dem GP Australie...  \n",
       "...                                                 ...  \n",
       "4443  Johannes Boie, Berlin Der deutschen Bundesregi...  \n",
       "4444  Brendan Lemieux (28) macht keinen Hehl daraus,...  \n",
       "4445  Prof. Stefan Homburg Hannover Nach dem Zerbrec...  \n",
       "4446  Bei den Trimbacher Gemeinderatswahlen tritt mi...  \n",
       "4447  Leise, freundlich, sympathisch – das war Walte...  \n",
       "\n",
       "[4448 rows x 9 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>pubtime</th>\n",
       "      <th>medium_code</th>\n",
       "      <th>medium_name</th>\n",
       "      <th>char_count</th>\n",
       "      <th>head</th>\n",
       "      <th>article_link</th>\n",
       "      <th>content_id</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>56411905</td>\n",
       "      <td>2025-03-20 10:25:38+01:00</td>\n",
       "      <td>NNTA</td>\n",
       "      <td>tagesanzeiger.ch</td>\n",
       "      <td>2406</td>\n",
       "      <td>Flughafen Zürich: Swiss-Flug nach Budapest mus...</td>\n",
       "      <td>https://www.tagesanzeiger.ch/flughafen-zuerich...</td>\n",
       "      <td>0031313b-f8f4-5281-70d8-9727db8ef8f6</td>\n",
       "      <td>Ein Flug der Swiss, durchgeführt von AirBaltic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>56360926</td>\n",
       "      <td>2025-03-14 17:44:58+01:00</td>\n",
       "      <td>NNBE</td>\n",
       "      <td>bernerzeitung.ch</td>\n",
       "      <td>1331</td>\n",
       "      <td>Zu hohe Temperaturen: Rennabbruch an der Bob-WM</td>\n",
       "      <td>https://www.bernerzeitung.ch/bob-wm-lake-placi...</td>\n",
       "      <td>003aa8b4-f90d-63f2-636c-33a3b91f0aea</td>\n",
       "      <td>Die Sonne sorgt an der Bob-WM für schwierige V...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>56349050</td>\n",
       "      <td>2025-03-13 14:53:13+01:00</td>\n",
       "      <td>SRF</td>\n",
       "      <td>srf.ch</td>\n",
       "      <td>2595</td>\n",
       "      <td>Mit den «Drachen» zum selben Höhenflug ansetze...</td>\n",
       "      <td>https://www.srf.ch/sport/eishockey/national-le...</td>\n",
       "      <td>007555d5-6e16-cb17-b0d0-213bc53cb964</td>\n",
       "      <td>2016 wurde Leuenberger mit dem SC Bern als Int...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>56370481</td>\n",
       "      <td>2025-03-15 23:35:40+01:00</td>\n",
       "      <td>NNBE</td>\n",
       "      <td>bernerzeitung.ch</td>\n",
       "      <td>1861</td>\n",
       "      <td>Eishockey-Playoffs: Lausanne muss den Ausgleic...</td>\n",
       "      <td>https://www.bernerzeitung.ch/eishockey-tigers-...</td>\n",
       "      <td>00a0ad13-aae5-160e-e51c-f7ddaa40841e</td>\n",
       "      <td>Lausanne verliert im zweiten Playoff-Viertelfi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>56372428</td>\n",
       "      <td>2025-03-16 09:41:18+01:00</td>\n",
       "      <td>SRF</td>\n",
       "      <td>srf.ch</td>\n",
       "      <td>3064</td>\n",
       "      <td>Norris' «stressiger» Sieg – Hülkenberg mit «Ba...</td>\n",
       "      <td>https://www.srf.ch/sport/motorsport/formel-1/s...</td>\n",
       "      <td>00e2d2b8-b04f-1963-5af5-60e5eeddf84f</td>\n",
       "      <td>Erleichterung bei Sauber nach dem GP Australie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4443</th>\n",
       "      <td>56340876</td>\n",
       "      <td>2025-03-13 00:00:00+01:00</td>\n",
       "      <td>NZZ</td>\n",
       "      <td>Neue Zürcher Zeitung</td>\n",
       "      <td>12728</td>\n",
       "      <td>Die Labor-These gewinnt wieder an Gewicht</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ffbb940a-c766-37ed-6f9b-bded485bf53b</td>\n",
       "      <td>Johannes Boie, Berlin Der deutschen Bundesregi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4444</th>\n",
       "      <td>56329575</td>\n",
       "      <td>2025-03-12 00:00:00+01:00</td>\n",
       "      <td>BLI</td>\n",
       "      <td>Blick</td>\n",
       "      <td>2737</td>\n",
       "      <td>Lemieux – der Mann fürs Grobe beim HCD</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ffcd9173-60dd-68ce-6603-36f63f76c0ca</td>\n",
       "      <td>Brendan Lemieux (28) macht keinen Hehl daraus,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4445</th>\n",
       "      <td>56345547</td>\n",
       "      <td>2025-03-13 00:00:00+01:00</td>\n",
       "      <td>WEW</td>\n",
       "      <td>Die Weltwoche</td>\n",
       "      <td>5286</td>\n",
       "      <td>Friedrich Merz und der Schulden-Tsunami</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ffce0fee-1078-691a-2e49-dcbf33efb38e</td>\n",
       "      <td>Prof. Stefan Homburg Hannover Nach dem Zerbrec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4446</th>\n",
       "      <td>56395580</td>\n",
       "      <td>2025-03-19 00:00:00+01:00</td>\n",
       "      <td>OLT</td>\n",
       "      <td>Oltner Tagblatt</td>\n",
       "      <td>3172</td>\n",
       "      <td>«Wollen machen statt jammern»</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ffd09e01-c429-b69f-cc68-2f68952469bf</td>\n",
       "      <td>Bei den Trimbacher Gemeinderatswahlen tritt mi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4447</th>\n",
       "      <td>56352301</td>\n",
       "      <td>2025-03-14 00:00:00+01:00</td>\n",
       "      <td>BLI</td>\n",
       "      <td>Blick</td>\n",
       "      <td>1840</td>\n",
       "      <td>Der letzte Schweizer Rad-Star der goldenen 50e...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ffe6a854-a7a2-1da9-fa81-577d7ca6c859</td>\n",
       "      <td>Leise, freundlich, sympathisch – das war Walte...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4448 rows × 9 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-22T18:27:13.020214Z",
     "start_time": "2025-03-22T18:23:38.800894Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.manifold import TSNE\n",
    "import plotly.express as px\n",
    "\n",
    "def df_plot_dbscan(df):\n",
    "    # Combine the header and the cleaned content\n",
    "    df = df.copy()  # Create a copy to avoid SettingWithCopyWarning\n",
    "    df.loc[:, \"combined_text\"] = df[\"head\"] + \" \" + df[\"content\"]\n",
    "\n",
    "    # Load a high-performance German-specific Sentence Transformer\n",
    "    model = SentenceTransformer('T-Systems-onsite/cross-en-de-roberta-sentence-transformer')\n",
    "\n",
    "    # Convert texts to embeddings\n",
    "    embeddings = model.encode(df['combined_text'].tolist())\n",
    "\n",
    "    # Initialize DBSCAN parameters\n",
    "    eps = 0.80\n",
    "    min_samples = 7\n",
    "    target_clusters = 4\n",
    "    max_clusters = 5\n",
    "    max_iterations = 100\n",
    "    iteration = 0\n",
    "\n",
    "    while iteration < max_iterations:\n",
    "        # DBSCAN Clustering with the current hyperparameters\n",
    "        dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "        df.loc[:, 'dbscan_cluster'] = dbscan.fit_predict(embeddings)\n",
    "\n",
    "        # Count the number of clusters (excluding noise points labeled as -1)\n",
    "        num_clusters = len(set(df['dbscan_cluster'])) - (1 if -1 in df['dbscan_cluster'] else 0)\n",
    "\n",
    "        if target_clusters <= num_clusters <= max_clusters:\n",
    "            break\n",
    "\n",
    "        # Adjust the eps value\n",
    "        if num_clusters > target_clusters:\n",
    "            eps = max(eps - 0.01, 0.01)  # Ensure eps does not go below 0.01\n",
    "        else:\n",
    "            eps += 0.01\n",
    "\n",
    "        iteration += 1\n",
    "\n",
    "    # Apply t-SNE for dimensionality reduction to 2 dimensions\n",
    "    tsne = TSNE(n_components=2, random_state=42)\n",
    "    tsne_results = tsne.fit_transform(embeddings)\n",
    "\n",
    "    # Add the t-SNE results and clusters to the DataFrame\n",
    "    df.loc[:, 'tsne_x'] = tsne_results[:, 0]\n",
    "    df.loc[:, 'tsne_y'] = tsne_results[:, 1]\n",
    "\n",
    "    # Create an interactive Plotly visualization for DBSCAN with the current parameters\n",
    "    fig_dbscan = px.scatter(\n",
    "        df[df[\"dbscan_cluster\"] >= 0],\n",
    "        x='tsne_x',\n",
    "        y='tsne_y',\n",
    "        color='dbscan_cluster',\n",
    "        hover_data=['head'],\n",
    "        title=f't-SNE Visualization of Text Clusters (DBSCAN: eps={dbscan.eps}, min_samples={dbscan.min_samples})'\n",
    "    )\n",
    "\n",
    "    for cluster_id in range(len(df['dbscan_cluster'].unique()) - 1):\n",
    "        cluster_data = df[df['dbscan_cluster'] == cluster_id]\n",
    "        print(f\"Cluster {cluster_id} heads:\")\n",
    "        for title in cluster_data['head'].tolist():\n",
    "            print(\"   \" + title)\n",
    "\n",
    "    # Show the DBSCAN plot\n",
    "    fig_dbscan.show()\n",
    "\n",
    "# Get unique dates from the pubtime column\n",
    "unique_dates = df['pubtime'].dt.date.unique()\n",
    "\n",
    "for date in unique_dates:\n",
    "    subset_df = df[df['pubtime'].dt.date == date]\n",
    "    print(f\"Topics of {date}:\")\n",
    "    df_plot_dbscan(subset_df)"
   ],
   "id": "1f276effbeb58092",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sfl\\Desktop\\Do-it_Einzugsgebiete\\.venv\\WAVE\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topics of 2025-03-20:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name T-Systems-onsite/cross-en-de-roberta-sentence-transformer. Creating a new one with mean pooling.\n",
      "C:\\Users\\sfl\\Desktop\\Do-it_Einzugsgebiete\\.venv\\WAVE\\lib\\site-packages\\huggingface_hub\\file_download.py:142: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\sfl\\.cache\\huggingface\\hub\\models--T-Systems-onsite--cross-en-de-roberta-sentence-transformer. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[3], line 78\u001B[0m\n\u001B[0;32m     76\u001B[0m subset_df \u001B[38;5;241m=\u001B[39m df[df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mpubtime\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mdt\u001B[38;5;241m.\u001B[39mdate \u001B[38;5;241m==\u001B[39m date]\n\u001B[0;32m     77\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTopics of \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mdate\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m:\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m---> 78\u001B[0m \u001B[43mdf_plot_dbscan\u001B[49m\u001B[43m(\u001B[49m\u001B[43msubset_df\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[3], line 16\u001B[0m, in \u001B[0;36mdf_plot_dbscan\u001B[1;34m(df)\u001B[0m\n\u001B[0;32m     13\u001B[0m model \u001B[38;5;241m=\u001B[39m SentenceTransformer(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mT-Systems-onsite/cross-en-de-roberta-sentence-transformer\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m     15\u001B[0m \u001B[38;5;66;03m# Convert texts to embeddings\u001B[39;00m\n\u001B[1;32m---> 16\u001B[0m embeddings \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mencode\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdf\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mcombined_text\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtolist\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     18\u001B[0m \u001B[38;5;66;03m# Initialize DBSCAN parameters\u001B[39;00m\n\u001B[0;32m     19\u001B[0m eps \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0.80\u001B[39m\n",
      "File \u001B[1;32m~\\Desktop\\Do-it_Einzugsgebiete\\.venv\\WAVE\\lib\\site-packages\\sentence_transformers\\SentenceTransformer.py:623\u001B[0m, in \u001B[0;36mSentenceTransformer.encode\u001B[1;34m(self, sentences, prompt_name, prompt, batch_size, show_progress_bar, output_value, precision, convert_to_numpy, convert_to_tensor, device, normalize_embeddings, **kwargs)\u001B[0m\n\u001B[0;32m    620\u001B[0m features\u001B[38;5;241m.\u001B[39mupdate(extra_features)\n\u001B[0;32m    622\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mno_grad():\n\u001B[1;32m--> 623\u001B[0m     out_features \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mforward(features, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    624\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdevice\u001B[38;5;241m.\u001B[39mtype \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhpu\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m    625\u001B[0m         out_features \u001B[38;5;241m=\u001B[39m copy\u001B[38;5;241m.\u001B[39mdeepcopy(out_features)\n",
      "File \u001B[1;32m~\\Desktop\\Do-it_Einzugsgebiete\\.venv\\WAVE\\lib\\site-packages\\sentence_transformers\\SentenceTransformer.py:685\u001B[0m, in \u001B[0;36mSentenceTransformer.forward\u001B[1;34m(self, input, **kwargs)\u001B[0m\n\u001B[0;32m    683\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: \u001B[38;5;28mdict\u001B[39m[\u001B[38;5;28mstr\u001B[39m, Tensor], \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mdict\u001B[39m[\u001B[38;5;28mstr\u001B[39m, Tensor]:\n\u001B[0;32m    684\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodule_kwargs \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m--> 685\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mforward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m    687\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module_name, module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnamed_children():\n\u001B[0;32m    688\u001B[0m         module_kwarg_keys \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodule_kwargs\u001B[38;5;241m.\u001B[39mget(module_name, [])\n",
      "File \u001B[1;32m~\\Desktop\\Do-it_Einzugsgebiete\\.venv\\WAVE\\lib\\site-packages\\torch\\nn\\modules\\container.py:250\u001B[0m, in \u001B[0;36mSequential.forward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    248\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m):\n\u001B[0;32m    249\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m:\n\u001B[1;32m--> 250\u001B[0m         \u001B[38;5;28minput\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[43mmodule\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m    251\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28minput\u001B[39m\n",
      "File \u001B[1;32m~\\Desktop\\Do-it_Einzugsgebiete\\.venv\\WAVE\\lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1737\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1738\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1739\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\Desktop\\Do-it_Einzugsgebiete\\.venv\\WAVE\\lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1745\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1746\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1747\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1748\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1749\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1750\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1752\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1753\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[1;32m~\\Desktop\\Do-it_Einzugsgebiete\\.venv\\WAVE\\lib\\site-packages\\sentence_transformers\\models\\Transformer.py:442\u001B[0m, in \u001B[0;36mTransformer.forward\u001B[1;34m(self, features, **kwargs)\u001B[0m\n\u001B[0;32m    435\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Returns token_embeddings, cls_token\"\"\"\u001B[39;00m\n\u001B[0;32m    436\u001B[0m trans_features \u001B[38;5;241m=\u001B[39m {\n\u001B[0;32m    437\u001B[0m     key: value\n\u001B[0;32m    438\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m key, value \u001B[38;5;129;01min\u001B[39;00m features\u001B[38;5;241m.\u001B[39mitems()\n\u001B[0;32m    439\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m key \u001B[38;5;129;01min\u001B[39;00m [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124minput_ids\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mattention_mask\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtoken_type_ids\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124minputs_embeds\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[0;32m    440\u001B[0m }\n\u001B[1;32m--> 442\u001B[0m output_states \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mauto_model(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mtrans_features, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs, return_dict\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[0;32m    443\u001B[0m output_tokens \u001B[38;5;241m=\u001B[39m output_states[\u001B[38;5;241m0\u001B[39m]\n\u001B[0;32m    445\u001B[0m \u001B[38;5;66;03m# If the AutoModel is wrapped with a PeftModelForFeatureExtraction, then it may have added virtual tokens\u001B[39;00m\n\u001B[0;32m    446\u001B[0m \u001B[38;5;66;03m# We need to extend the attention mask to include these virtual tokens, or the pooling will fail\u001B[39;00m\n",
      "File \u001B[1;32m~\\Desktop\\Do-it_Einzugsgebiete\\.venv\\WAVE\\lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1737\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1738\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1739\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\Desktop\\Do-it_Einzugsgebiete\\.venv\\WAVE\\lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1745\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1746\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1747\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1748\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1749\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1750\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1752\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1753\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[1;32m~\\Desktop\\Do-it_Einzugsgebiete\\.venv\\WAVE\\lib\\site-packages\\transformers\\models\\xlm_roberta\\modeling_xlm_roberta.py:977\u001B[0m, in \u001B[0;36mXLMRobertaModel.forward\u001B[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[0;32m    970\u001B[0m \u001B[38;5;66;03m# Prepare head mask if needed\u001B[39;00m\n\u001B[0;32m    971\u001B[0m \u001B[38;5;66;03m# 1.0 in head_mask indicate we keep the head\u001B[39;00m\n\u001B[0;32m    972\u001B[0m \u001B[38;5;66;03m# attention_probs has shape bsz x n_heads x N x N\u001B[39;00m\n\u001B[0;32m    973\u001B[0m \u001B[38;5;66;03m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001B[39;00m\n\u001B[0;32m    974\u001B[0m \u001B[38;5;66;03m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001B[39;00m\n\u001B[0;32m    975\u001B[0m head_mask \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mget_head_mask(head_mask, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39mnum_hidden_layers)\n\u001B[1;32m--> 977\u001B[0m encoder_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mencoder\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    978\u001B[0m \u001B[43m    \u001B[49m\u001B[43membedding_output\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    979\u001B[0m \u001B[43m    \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mextended_attention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    980\u001B[0m \u001B[43m    \u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhead_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    981\u001B[0m \u001B[43m    \u001B[49m\u001B[43mencoder_hidden_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mencoder_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    982\u001B[0m \u001B[43m    \u001B[49m\u001B[43mencoder_attention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mencoder_extended_attention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    983\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpast_key_values\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpast_key_values\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    984\u001B[0m \u001B[43m    \u001B[49m\u001B[43muse_cache\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muse_cache\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    985\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    986\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_hidden_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    987\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_dict\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    988\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    989\u001B[0m sequence_output \u001B[38;5;241m=\u001B[39m encoder_outputs[\u001B[38;5;241m0\u001B[39m]\n\u001B[0;32m    990\u001B[0m pooled_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpooler(sequence_output) \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpooler \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32m~\\Desktop\\Do-it_Einzugsgebiete\\.venv\\WAVE\\lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1737\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1738\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1739\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\Desktop\\Do-it_Einzugsgebiete\\.venv\\WAVE\\lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1745\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1746\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1747\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1748\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1749\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1750\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1752\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1753\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[1;32m~\\Desktop\\Do-it_Einzugsgebiete\\.venv\\WAVE\\lib\\site-packages\\transformers\\models\\xlm_roberta\\modeling_xlm_roberta.py:632\u001B[0m, in \u001B[0;36mXLMRobertaEncoder.forward\u001B[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[0;32m    621\u001B[0m     layer_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_gradient_checkpointing_func(\n\u001B[0;32m    622\u001B[0m         layer_module\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__call__\u001B[39m,\n\u001B[0;32m    623\u001B[0m         hidden_states,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    629\u001B[0m         output_attentions,\n\u001B[0;32m    630\u001B[0m     )\n\u001B[0;32m    631\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 632\u001B[0m     layer_outputs \u001B[38;5;241m=\u001B[39m \u001B[43mlayer_module\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    633\u001B[0m \u001B[43m        \u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    634\u001B[0m \u001B[43m        \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    635\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlayer_head_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    636\u001B[0m \u001B[43m        \u001B[49m\u001B[43mencoder_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    637\u001B[0m \u001B[43m        \u001B[49m\u001B[43mencoder_attention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    638\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpast_key_value\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    639\u001B[0m \u001B[43m        \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    640\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    642\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m layer_outputs[\u001B[38;5;241m0\u001B[39m]\n\u001B[0;32m    643\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m use_cache:\n",
      "File \u001B[1;32m~\\Desktop\\Do-it_Einzugsgebiete\\.venv\\WAVE\\lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1737\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1738\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1739\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\Desktop\\Do-it_Einzugsgebiete\\.venv\\WAVE\\lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1745\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1746\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1747\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1748\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1749\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1750\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1752\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1753\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[1;32m~\\Desktop\\Do-it_Einzugsgebiete\\.venv\\WAVE\\lib\\site-packages\\transformers\\models\\xlm_roberta\\modeling_xlm_roberta.py:521\u001B[0m, in \u001B[0;36mXLMRobertaLayer.forward\u001B[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001B[0m\n\u001B[0;32m    509\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mforward\u001B[39m(\n\u001B[0;32m    510\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m    511\u001B[0m     hidden_states: torch\u001B[38;5;241m.\u001B[39mTensor,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    518\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tuple[torch\u001B[38;5;241m.\u001B[39mTensor]:\n\u001B[0;32m    519\u001B[0m     \u001B[38;5;66;03m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001B[39;00m\n\u001B[0;32m    520\u001B[0m     self_attn_past_key_value \u001B[38;5;241m=\u001B[39m past_key_value[:\u001B[38;5;241m2\u001B[39m] \u001B[38;5;28;01mif\u001B[39;00m past_key_value \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m--> 521\u001B[0m     self_attention_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mattention\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    522\u001B[0m \u001B[43m        \u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    523\u001B[0m \u001B[43m        \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    524\u001B[0m \u001B[43m        \u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    525\u001B[0m \u001B[43m        \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    526\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpast_key_value\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mself_attn_past_key_value\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    527\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    528\u001B[0m     attention_output \u001B[38;5;241m=\u001B[39m self_attention_outputs[\u001B[38;5;241m0\u001B[39m]\n\u001B[0;32m    530\u001B[0m     \u001B[38;5;66;03m# if decoder, the last output is tuple of self-attn cache\u001B[39;00m\n",
      "File \u001B[1;32m~\\Desktop\\Do-it_Einzugsgebiete\\.venv\\WAVE\\lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1737\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1738\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1739\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\Desktop\\Do-it_Einzugsgebiete\\.venv\\WAVE\\lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1745\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1746\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1747\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1748\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1749\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1750\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1752\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1753\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[1;32m~\\Desktop\\Do-it_Einzugsgebiete\\.venv\\WAVE\\lib\\site-packages\\transformers\\models\\xlm_roberta\\modeling_xlm_roberta.py:448\u001B[0m, in \u001B[0;36mXLMRobertaAttention.forward\u001B[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001B[0m\n\u001B[0;32m    438\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mforward\u001B[39m(\n\u001B[0;32m    439\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m    440\u001B[0m     hidden_states: torch\u001B[38;5;241m.\u001B[39mTensor,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    446\u001B[0m     output_attentions: Optional[\u001B[38;5;28mbool\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[0;32m    447\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tuple[torch\u001B[38;5;241m.\u001B[39mTensor]:\n\u001B[1;32m--> 448\u001B[0m     self_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mself\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    449\u001B[0m \u001B[43m        \u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    450\u001B[0m \u001B[43m        \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    451\u001B[0m \u001B[43m        \u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    452\u001B[0m \u001B[43m        \u001B[49m\u001B[43mencoder_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    453\u001B[0m \u001B[43m        \u001B[49m\u001B[43mencoder_attention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    454\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpast_key_value\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    455\u001B[0m \u001B[43m        \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    456\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    457\u001B[0m     attention_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moutput(self_outputs[\u001B[38;5;241m0\u001B[39m], hidden_states)\n\u001B[0;32m    458\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m (attention_output,) \u001B[38;5;241m+\u001B[39m self_outputs[\u001B[38;5;241m1\u001B[39m:]  \u001B[38;5;66;03m# add attentions if we output them\u001B[39;00m\n",
      "File \u001B[1;32m~\\Desktop\\Do-it_Einzugsgebiete\\.venv\\WAVE\\lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1737\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1738\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1739\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\Desktop\\Do-it_Einzugsgebiete\\.venv\\WAVE\\lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1745\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1746\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1747\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1748\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1749\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1750\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1752\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1753\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[1;32m~\\Desktop\\Do-it_Einzugsgebiete\\.venv\\WAVE\\lib\\site-packages\\transformers\\models\\xlm_roberta\\modeling_xlm_roberta.py:326\u001B[0m, in \u001B[0;36mXLMRobertaSdpaSelfAttention.forward\u001B[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001B[0m\n\u001B[0;32m    314\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39mforward(\n\u001B[0;32m    315\u001B[0m         hidden_states,\n\u001B[0;32m    316\u001B[0m         attention_mask,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    321\u001B[0m         output_attentions,\n\u001B[0;32m    322\u001B[0m     )\n\u001B[0;32m    324\u001B[0m bsz, tgt_len, _ \u001B[38;5;241m=\u001B[39m hidden_states\u001B[38;5;241m.\u001B[39msize()\n\u001B[1;32m--> 326\u001B[0m query_layer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtranspose_for_scores(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mquery\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[0;32m    328\u001B[0m \u001B[38;5;66;03m# If this is instantiated as a cross-attention module, the keys and values come from an encoder; the attention\u001B[39;00m\n\u001B[0;32m    329\u001B[0m \u001B[38;5;66;03m# mask needs to be such that the encoder's padding tokens are not attended to.\u001B[39;00m\n\u001B[0;32m    330\u001B[0m is_cross_attention \u001B[38;5;241m=\u001B[39m encoder_hidden_states \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32m~\\Desktop\\Do-it_Einzugsgebiete\\.venv\\WAVE\\lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1737\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1738\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1739\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\Desktop\\Do-it_Einzugsgebiete\\.venv\\WAVE\\lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1745\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1746\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1747\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1748\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1749\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1750\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1752\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1753\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[1;32m~\\Desktop\\Do-it_Einzugsgebiete\\.venv\\WAVE\\lib\\site-packages\\torch\\nn\\modules\\linear.py:125\u001B[0m, in \u001B[0;36mLinear.forward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    124\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[1;32m--> 125\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlinear\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 3
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
